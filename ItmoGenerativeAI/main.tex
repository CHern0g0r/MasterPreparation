%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
% \documentclass[14pt]{extarticle}
\usepackage{pdfpages}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Generative AI} % Title of the assignment

%----------------------------------------------------------------------------------------

\DeclareMathOperator*\uplim{\overline{lim}}


\begin{document}

\maketitle % Print the title
\section{Математика и анализ данных}

\subsection{Числовые ряды. Абсолютная и условная сходимость. Признаки сходимости числовых рядов.}

\subsubsection{Числовые ряды}

$\sum\limits_{k=1}^{\infty} a_{k} = a_{1} + a_{2} + a_{3} + \cdots$ -- числовой ряд

Сходимость ряда означает существование конечной суммы, т.е. $\sum\limits_{k=1}^{\infty} a_{k} = S$ где $S$ -- конечное число, иначе ряд считается расходящимся.

\subsubsection{Абсолютная и условная сходимость}

Ряд $\sum\limits_{k=1}^{\infty} a_{k}$ называется {\bf абсолютно} сходящимся, если сходится ряд из модулей $\sum\limits_{k=1}^{\infty} |a_{k}|$, иначе ряд называется {\bf условно} сходящимся

\subsubsection{Признаки сходимости числовых рядов} 

{\bf Знакоположительные ряды} (ряды с положительными членами):

Критерий сходимости знакоположительных рядов-- знакоположительный ряд $\sum\limits_{k=1}^{\infty} a_{k}$ сходится тогда и только тогда, когда последовательность его частичных сумм $S(n) = \sum\limits_{k=1}^{k=n}a_{k}$ ограничена сверху

{\bf Док-во:}

=>: ряд сходится, значит последовательность частичных сумм $\S(n) =\sum\limits_{k=1}^{n} a_{k}$ имеет предел равный $\sum\limits_{k=1}^{\infty} a_{k} = S$

<=: Пусть дан положительный ряд и последовательность частичных сумм ограничена сверху, заметим что последовательность частичных сумм неубывающая:
$$S_{n + 1} - S_{n} = a_{n + 1} \ge 0$$. Используя свойство из теоремы о монотонной последовательности получаем, что т.к. последовательность частичных сумм монотонно не убывает и ограничена сверху, значит она сходится и потому ряд сходится по определению.

{\bf Признак сравнения с мажорантой}

Пусть даны два положительных ряда $\sum\limits_{k=1}^{\infty} a_{k}$ и $\sum\limits_{k=1}^{\infty} b_{k}$. Если начиная с некоторого номера $n > N$ выполняется неравенство $0 \le a_n \le b_n$, то:

\begin{itemize}
	\item из сходимости рядя $\sum\limits_{k=1}^{\infty} b_{k}$ следует сходимость ряда $\sum\limits_{k=1}^{\infty} a_{k}$
	\item из расходимости ряда $\sum\limits_{k=1}^{\infty} a_{k}$ следует расходимость $\sum\limits_{k=1}^{\infty} b_{k}$
\end{itemize}

{\bf Док-во:}

Из неравенств на члены следует неравенство на частичные суммы $0 \le S_n \le \sigma_n$, дальше очев.


{\bf Признак Раабе}

Если для ряда $\sum\limits_{k=1}^{\infty} a_{k}$ существует предел $$R = \lim\limits_{n \rightarrow \infty} n (\frac{a_n}{a_{n+1}} - 1)$$, то при $R > 1$ ряд сходится, а при $R < 1$ -- расходится. Если $R = 1$, то жанный признак не говорит ничего.

{\bf Признак Гаусса}

Пусть для знакоположительного ряда $\sum\limits_{n=1}^{\infty} a_{n}$ отношение $\frac{a_n}{a_{n + 1}}$ может быть представлено в виде $$\frac{a_n}{a_{n + 1}} = \lambda + \frac{\mu}{n} + \frac{\theta_n}{n^2}$$, где $\lambda, \mu$ -- постоянные, а последовательность $\theta_n$ ограничена. Тогда 
\begin{itemize}
	\item ряд расходится если либо $\lambda > 1$, либо $\lambda = 1, \mu > 1$
	\item ряд расходится, если либо $\lambda < 1$, либо $\lambda = 1, \mu \le 1$
\end{itemize}


{\bf Знакопеременные ряды}

\D{Знакопеременными называются ряды, члены которых могут (стоять) быть как положительными, так и отрицательными.}


{\bf Признак Даламбера}

Слабее признака Коши, но зато проще

Если существует $\lim\limits_{n \rightarrow \infty}|\frac{a_{n + 1}}{a_n}| = r$, то 

\begin{itemize}
	\item если $r < 1$, то ряд абсолютно сходится
	\item если $r > 1$, то ряд расходится
	\item если $r = 1$, то данный признак ничего не говорит (сука)
\end{itemize}

{\bf Док-во:}

1. Пусть начиная с некоторого номера N верно неравенство $|\frac{a_{n+1}}{a_n}| \le q, 0 < q < 1$. Тогда перемножив члены начиная с N будем иметь что $\frac{a_{N+n}}{a_N} \le q^n$ откуда $|a_{N+n}| \le |a_{N}q^n|$, значит ряд $|a_{N+1}| + |a_{N+2}| + ...$ меньше бесконечной суммы убывающей геометрической прогрессии, поэтому он сходится

2. $|\frac{a_{n + 1}}{a_n}| \ge 1$ (с некоторого N), тогда можно записать $|a_{n+1}| \ge |a_n|$ значит модуль членов $a$ не стремится к 0 на бесконечности, значит последовательность не стремится к 0 а значит ряд не сходится.

3. Если просто меньше 1 до там хуйня какая-то мне впадлу
\\

{\bf Радикальный признак Коши} (ебаная оппозиция)

Если существует $\lim\lim\limits_{n \rightarrow \infty} \sqrt[n]{|a_n|} = r$, то

\begin{itemize}
	\item если $r < 1$ то ряд сходится абсолютно
	\item если $r > 1$ то ряд расходится
	\item если $r = 1$ то хз (опять??)
\end{itemize}

{\bf Док-во:} \href{https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B4%D0%B8%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA_%D0%9A%D0%BE%D1%88%D0%B8}{тут}
\\

{\bf Признак Лейбница}

Пусть для знакочередующегося ряда $$S = \sum\limits_{n=1}^{\infty}(-1)^{n-1}a_n, a_n \ge 0$$
выполняются следующие условия

\begin{itemize}
	\item С некоторого $N$ последовательность $a$ монотонно убывает, т.е. $a_{n+1} \le a_n$
	\item $\lim\limits_{n \rightarrow \infty}a_n = 0$
\end{itemize}

Тогда такой ряд сходится

{\bf Док-во:} \href{https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BD%D0%B8%D1%86%D0%B0_%D0%BE_%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8_%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D1%87%D0%B5%D1%80%D0%B5%D0%B4%D1%83%D1%8E%D1%89%D0%B8%D1%85%D1%81%D1%8F_%D1%80%D1%8F%D0%B4%D0%BE%D0%B2}{здесь}\\
	
{\bf Признак Абеля}

\T {Числовой ряд $\sum\limits_{n=1}^{\infty}a_nb_n$ сходится, если выполнены следующие условия

\begin{itemize}
	\item Последовательность \{$a_n$\} монотонна и ограничена
	\item Ряд $\sum\limits_{n=1}^{\infty}b_n$ сходится
\end{itemize}
}
{\bf Proof:} \href{https://ib.mazurok.com/2015/06/16/%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B8-%D0%B0%D0%B1%D0%B5%D0%BB%D1%8F-%D0%B8-%D0%B4%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5/}{вот}\\
	
{\bf Признак Дирихле}

\T{Пусть выполнены условия:
\begin{itemize}
	\item последовательность частичных сумм $B_n = \sum\limits_{k=1}^{n}$ ограничена
	\item последовательность $a_n$, начиная с некоторого номера, монотонно убывает $a_n \ge a_{n+1}$
	\item $\lim\limits_{n\rightarrow\infty}a_n = 0$
\end{itemize}
Тогда ряд $\sum\limits_{n=1}^{\infty}a_nb_b$ сходится
}
	
{\bf Proof:} \href{https://ib.mazurok.com/2015/06/16/%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B8-%D0%B0%D0%B1%D0%B5%D0%BB%D1%8F-%D0%B8-%D0%B4%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5/}{вот}\\

\subsection{Кратные, поверхностные и криволинейные интегралы. Формулы Грина, Стокса и Остроградского}


\subsubsection{Интеграль4ики}
\D{Пусть дана $f(x)$ -- функция действительной переменной. {\bf Неопределенным интегралом} функции $f(x)$, или ее первообразной, называется такая функция $F(x)$, производная которой равна $f(x)$, т.е. $F^{'}(x) = f(x)$. Обозначается $F(x) = \int f(x)dx$ }


\D{Кратным интегралом называют множество интегралов, взятых от $d > 1$, например  $$\underbrace{\int...\int f(x_1,...,x_d)dx_{1}...dx_{d}}_{d}$$}

Замечание -- кратный интеграл -- определенный интеграл, при его вычислении всегда получается число

\D{Криволинейный интеграл -- интеграл вычисляемый вдоль какой-либо прямой.

Пусть $l$ -- пгладкая, без особых точек и пересечений кривая (может быть замкнутой), заданая параметрически $l: r(t)$, где $r$ -- радиус вектор, конец которого описывает кривую, а параметр $t$ направлен от начального значения $a$ к конечному значению $b$. Для интеграла второго рода направление, в котором движется параметр, определяет направление кривой $l$.

Также есть скалярная или векторная функция , которая рассматривается вдоль кривой $l: f(r)$

Еще есть разбиение отрезка параметризации, и разбиение кривой. Они соответствуют друг-другу (параметризация от параметра по факту сопостовляет точке из отрезка параметризации $[a, b]$ точку на прямой, и по разбиению параметризации разбивается кривая по соответствующим точкам, подробнее можно почитать на вики ссылку вставить не получилось:( )

Интегральная сумма для интеграла {\bf первого рода} -- сумма вида $\sum\limits_{k=1}^{n} f(r(\xi_i))\cdot|l_k|$ где $|l_k|$ -- длина соответствующего отрезка, $\xi_i$ -- точка на соответствующем отрезке

Интегральная сумма для интеграла {\bf первого рода} -- сумма вида $\sum\limits_{k=1}^{n} f(r(\xi_i))\cdot(r(t_k) = r(t_{k - 1})))$

Собственно, криволинейный интеграл это интегральная сумма с $n$ устремленным в бесконечность
}

Похоже на обычный определенный интеграл, только тут мы вместо оси выравниваемся на кривую какую-то, и по факту считаем площидь криволинейного цилиндра между кривой в пространстве и ее проекцией (вроде бы, но это не точно)


\D{Пусть $\Phi$ -- гладкая, ограниченная полная поверхность. Пусть далее на $\Phi$ задана функция $f(M) = f(x, y, z)$. Рассмотрим разбиение $T$ этой поверхности на часть $\Phi_i (i=1, ..., n)$ кусочно-гладкими кривыми и на каждой такой части выберем произвольную точку $M_i(x_i, y_i, z_i)$. Вычислив значение функции в этой точке $f(M_i) = f(x_i, y_i, z_i)$ и, приняв за $\sigma_i$ площадь поверхность $\Phi_i$, рассмотрим сумму $$I\{\Phi_i, M_i\} = \sum_i f(M_i)\sigma_i$$. Тогда число I называется пределом сумм $i\{\Phi_i, M_i\}$ если $$\forall \epsilon > 0 \exists \delta > 0 \forall T: d(T) < \delta \forall \{M_i\} |I\{\Phi_i, M_i\} - I| < \epsilon$$
Предел $I$ сумм $I\{\Phi_i, M_i\}$ при $d(T) \rightarrow 0$
 называется {\bf поверхностным интегралом первого рода} от функции $f(M)$ по поверхности $\Phi$ и обозначается $$I = \iint\limits_{\Phi} f(M)d\sigma$$} 

По сути -- берем поверхность в пространстве, а дальше как в криволинейном -- вместо отрезков оже куски пространства и т.д. получается магия какая-то.

\subsubsection{Формула Грина}

\D{Пусть $C$ -- положительно ориентированная кусочно-гладкая замкнутая кривая на плоскости, а $D$ -- область, ограниченная кривой $C$. Если фунеции $P = P(x, y)$, $Q = Q(x, y)$ определены в области $D$ и имеют неприрывные частные производные $\frac{\partial P}{\partial y}, \frac{\partial Q}{\partial x}$, то $$\oint Pdx + Qdy = \iint\limits_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy$$}
{\bf Док-во и еще:} \href{https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%93%D1%80%D0%B8%D0%BD%D0%B0}{here}
	
\subsubsection{Формула Стокса}

\D{Пусть на ориентируемом многообразии $M$ размерности $n$ заданы положительно ориентированное ограниченное $p-$мерное подмногообразие $\sigma (1 \le p \le n)$ и дифференциальная форма $\omega$ степени $p - 1$ класса $C^1$. Тогда если граница подмногообразия $\partial \sigma$ положительно ориентированаб то $$\int\limits_{\sigma}d\omega = \int\limits_{\partial \sigma \omega}$$}

Грубо говоря взяли поверхность, и с помощью дифференциалов перешли к интегралу по границе поверхности, как-то так, но надо глубже разбираться потому что очень много определений которые надо помнить

\subsubsection{Формула Остроградского (Гаусс сосать)}

\D{Пусть теперь $\partial V$ -- кусочно-гладкая гипперповерхность $(p = n - 1)$, ограничивающая некоторую область $V$ в $n-$мерном пространстве. Тогда интеграл дивергенции (это оператор который отображает векторное поле на скалярное -- $div F = \lim\limits_{V \rightarrow 0} \frac{\Phi_F}{V}$, где $\Phi_F$ -- поток векторного поля $F$ через сферическую поверхность площадью $S$ ограничивающую объем $V$, хуита какая-то хочу объяснение на пальцах) поля по области равен потоку поля через границу области $\partial V$: $$\int\limits_{V} div F dV = \int\limits_{\partial V} F d \Sigma$$.
	
В трехмерном пространстве $(n = 3)$ с координатами $\{x, y, z\}$ эквивалентнно $$\int\limits_{\partial V} F d \Sigma = \int\limits_{V}(\frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z}) dV$$, или $$\iiint\limits_{\partial V} Pdydz + Qdzdx + Rdxdy = \iint\limits_{V} (\frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z})dxdydz$$ }


Понятно что тут взяли и применили стокса на какой-то случай, но чет пиздец ребята)))


\subsection{Функциональные ряды, свойства равномерно сходящихся функциональных рядов. Степенные ряды. Ряд Тейлора.}

\subsubsection{Функциональные ряды}

\D {Функциональный ряд -- ряд, каждым членом которого является функция $u_k(x)$

Обозначается $\sum\limits_{k=1}^{\infty} u_k(x)$}

Функциональная последовательность $u_k(x)$ сходится {\bf поточечно} к функции $u(x)$, если $\forall x \in E \exists \lim\limits_{k \rightarrow \infty} u_k(x) = u(x)$

{\bf Равномерная сходимость} -- существует функция $u(x): E \rightarrow \mathbb{C}$ такая, что 

$sup |u_k(x) - u(x)| \xrightarrow {k \rightarrow \infty} 0, x \in E$

Функциональный ряд называется сходящимся {\bf поточечно}, если последовательность $S_n(x) = \sum\limits_{k=1}^{n} u_k(n)$ сходится поточечно. Аналогично для равномерной сходимости.

{\bf Необходимое условие равноменой сходимости ряда}

$u_k(x) \rightrightarrows 0$ при $k \rightarrow \infty$

Или, что эквивалентно $\forall \epsilon > 0 \exists n_0(\epsilon) \in \mathbb{N} : \forall x \in X, \forall n > n_0 |u_n(x)| < \epsilon$, где $X$ -- область сходимости

{\bf Свойства}

\begin{enumerate}
	\item {\bf Теоремы о непрерывности}
	
	Последовательность непрерывных в точке функций сходится к функции, непрерывной в этой точке.
	
	Последовательность $u_k(x) \rightrightarrows u(x)$
	
	$\forall k:$ функция $u_k(x)$ непрерывна в точке $x_0$
	
	Тогда и $u(x)$ непрерывна в $x_0$
	
	Ряд непрерывных в точке функций сходится к функции, непрерывной в этой точке.
	
	Ряд $\sum\limits_{k=0}^{\infty}u_k(x) \rightrightarrows S(x)$
	
	$\forall k$: функция непрерывна в точкке $x_0$
	
	Тогда $S(x)$ непрерывна в  $x_0$
	
	\item {\bf Теоремы об интегрировании}
	
	Рассматриваются действительнозначные функции на отрезке действительной оси
	
	{\it Теорема о переходе к пределу под знаком интеграла}
	
	$\forall k:$ функция $u_k(x)$ непрерывна на отрезке $[a, b]$
	
	$u_k(x) \rightrightarrows u(x)$ на $[a, b]$
	
	Тогда числовая последовательность $\{\int\limits_{a}^{b} u_k(x) dx\}$ сходится к конечному пределу $\int\limits_a^b u(x) dx$
	
	{\it Теорема о почленном интегрировании}
	
	$\forall k:$ функция $u_k(x)$ непрерывна на отрезке $[a, b]$
	
	$\sum\limits_{k=1}^{\infty}u_k(x) \rightrightarrows S(x)$ на $[a, b]$
	
	Тогда числовой ряд $\sum\limits_{k=1}^{\infty}\int\limits_{a}^{b} u_k(x) dx$ сходится и равен $\int\limits_a^b S(x) dx$
	
	\item {\bf Теоремы о дифференцировании}
	
	Рассматриваются действительнозначные функции на отрезке действительной оси
	
	{\it Теорема о дифференцировании под пределом}
	
	$\forall k:$ функция $u_k(x)$ дифференцируема (имеет непрерывную производную) на отрезке $[a, b]$
	
	$\exists c \in [a, b]: u_k(c)$ сходится к конечному пределу
	
	$u_k^{\prime}(x)  \rightrightarrows \omega(x)$ на отрезке $[a, b]$
	
	Тогда $\exists u(x): u_k(x) \rightrightarrows u(x),\ u(x)$ -- дифференцируема на $[a, b],\ u^{\prime}(x) = \omega(x)$ на $[a, b]$
	
	{\it Теорема о почленном дифференцировании}

	$\forall k:$ функция $u_k(x)$ -- дифференцируема на отрезке $[a, b]$
	
	$\exists c \in [a, b]: \sum\limits_{k=1}^{\infty} u_k(c)$ сходится
	
	$\sum\limits_{k=1}^{\infty}u_k^{\prime}(x)$ равномерно сходится на отрезке $[a, b]$
	
	Тогда $\exists S(x): \sum\limits_{k=1}^{\infty}u_k(x) \rightrightarrows S(x),\ S(x)$ -- дифференцируем на $[a, b], S^{\prime}(x) = \sum\limits_{k=1}^{\infty}u_k^{\prime}(x)$ на $[a, b]$
	
\end{enumerate}

\subsubsection{Степенные ряды}

\D {{\bf Степенной ряд с одной переменной} -- это формальное алгебраическое вырадение вида $$F(x) = \sum\limits_{n=0}^{\infty}a_nX^n$$ в котором коэффициенты $a_n$ берутся из некоторого кольца $R$, обычно вещественные или комплексные числа}

Для степенных рядов есть несколько теорем об их сходимости

\begin{itemize}
	\item Певая теорема Абеля
	
	Пусть ряд $\sum a_n x^n$ сходится в точке $x_0$. Тогда этот ряд сходится абсолютно в круге $|x| < |x_0|$ и равномерно по $x$ на любом компактном подмножестве этого круга.
	
	Отсюда можно сделать вывод что если ряд расходится при $x = x_0$, то он расходится при всех $|x| > |x_0|$
	
	Появляется понятие радиуса сходимости $R$, при котором при $|x| < R$ ряд сходится абсолютно, про $|x| > R$ расходится
	
	\item Формула Коши-Адамара (Коши-Амидамару)
	
	 Значение радиуса сходимости степенного ряда может быть вычислено по формуле $\frac{1}{R} = \uplim\limits_{n \rightarrow +\infty}|a_n|^{1/n}$
	  
	 \item Признак Даламбера
	 
	 Если при $n > N$ и $\alpha > 1$ выполнено неравенство $|\frac{a_n}{a_{n+1}}| \ge R(1 + \frac{\alpha}{n})$ тогда степенной ряд $\sum a_n x^n$ сходится во всех точках окружности $|x| = R$ абсолютно и равномерно по $x$
	 
	 \item Признак Дирихле
	 
	 Если все коэффициенты степенного ряда $\sum a_n x^n$ положительны и последовательность $a_n$ монотонно сходится к 0б тогда этот ряд сходится во всех точках окружности $|x| = 1$, кроме, может быть, точки $x = 1$
\end{itemize}


\subsubsection{Ряд Тейлора}

\D {Ряд Тейлора -- разложение функции в бесконечную сумму степенных функций

Многочленом Тейлора функции $f(x)$ вещественной переменной $x$, дифференцируемой $k$ раз в точке $a$ называется конечная сумма 
$$f(x) = \sum\limits_{n=0}{k}\frac{f^{(n)}(a)}{n!} (x - a)^n = f(a) + f^{\prime}(a)(x - a) + \frac{f^{(2)}(a)}{2!}(x - a)^2 + ... + \frac{f^{(k)}(a)}{k!}(x - k)^k$$

Рядом Тейлора в точке $a$ функции $f(x)$ , бесконечно диффиренцируемой в окрестности точки $a$, называется формальный степенной ряд
$$f(x) = \sum\limits_{n=0}^{+\infty}\frac{f^{(n)}(a)}{n!}(x - a)^n$$

Другими словами, рядом Тейлора функции $f(x)$ в точке $a$ называется ряд разложения функции по положительным степеням двучлена $(x - a)$

}

Еще есть формула Тейлора, это просто частичная сумма ряда вроде как.

В случае $a = 0$ это все безобразие -- {\bf ряд Маклорена}


\subsection{Определители и их свойства. Системы линейных алгебраических уравнений и их исследование. Методы решения систем линейных алгебраических уравнений.}

\subsubsection{Определитель}

\D {Определитель -- скалярная величина, которая характеризует ориентированное "растяжение"\ или "сжатие" \  многомерного евклидова пространства после преобразования матрицей. Имеет смысл только для квадратных матриц. Стандартные обозначения -- $\det(A), |A|, \Delta (A)$ }

{\bf Определение через перестановки}

Для квадратной матрицы $A = (a_{ij})$ размера $n \times n$ ее определитель вычисляется по формуле $$det A = \sum\limits_{\alpha_1, \alpha_2, ..., \alpha_n}(-1)^{N(\alpha_1, \alpha_2, ..., \alpha_n)} \cdot a_{1 \alpha_1} a_{2 \alpha_2} ... a_{n \alpha_n}$$

Где суммирование проводится по всем перестановкам $\alpha_1, \alpha_2, ..., \alpha_n$ чисел $1, 2, ..., n$, а $N(\alpha_1, \alpha_2, ..., \alpha_n)$ обозначает число инверсий в перестановке $\alpha_1, ..., \alpha_n$

Таким образом в определитель входит $n!$ слагаемых.


{\bf Аксиоматическое построение}

Понятие определителя может быть введено на основе его свойств. А именно, определителем вещественной матрицы называется функция $det: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$, обладающая следующими тремя свойствами

\begin{itemize}
	\item $\det(A)$ -- кососимметрическая функция строк(столбцов) матрицы $A$, т.е. не меняется при четных перестановках аргументов
	\item $\det(A)$ -- полилинейная функция строк (столбцов) матрицы $A$
	\item $\det(E) = 1$, где $E$ -- единичная $n \times n$ матрица.
\end{itemize}

Еще свойства

\begin{enumerate}
	\item $\det E = 1$
	\item $\det cA = c^n \det A$
	\item $\det A^T = \det A$
	\item $\det(AB) = \det A \cdot \det B$
	\item $\det A^{-1} = (\det A)^{-1}$, причем матрица обратима тогда и только тогда, когда обратим ее определитель
	\item Существует ненулевое решение уравнения $AX = 0$ тогда и только тогда, когда $\det A = 0$ 
\end{enumerate}

\subsubsection{Системы линейных уравнений}

В классическом варианте коэффициенты при переменных, свободные члены и неизвестные считаются вещественными числами

Общий вид системы линейных алгебраических уравнений:

$$
{\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\dots +a_{1n}x_{n}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\dots +a_{2n}x_{n}=b_{2}\\\dots \\a_{m1}x_{1}+a_{m2}x_{2}+\dots +a_{mn}x_{n}=b_{m}\\\end{cases}}$$

где $m$ -- количество уравнений, а $n$ -- количество переменных.

Система называется {\bf однородной}, если все ее свободные члены ($b_i$) равны нулю, иначе -- {\bf неоднородной}

Система называется {\bf совместной}, если она имеет хотя бы одно решение, иначе несовместной. Решения считаются различными, если хотя бы одно из значений переменных не совпадает. Если решение одно, то система {\bf определенная}

Также есть запись в матричной форме

$
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{pmatrix}
\begin{pmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
\end{pmatrix} 
=
\begin{pmatrix}
	b_1 \\
	b_2 \\
	\vdots \\
	b_m
\end{pmatrix}$

или $Ax=b$. Если к матрицу $A$ приписать справа столбец свободных членов, матрица будет называться расширенной.

Системы называются {\bf эквивалентными}, если множество их решений совпадает, т.е. если решение одной системы является решением другой.

Можно менять уравнения домножением на константу кроме 0, на сумму с другим уравнением, на линейную комбинацию с учетом этой. Будут получаться эквивалентные системы.

\subsubsection{Методы решения систем уравнений}

\begin{itemize}
	\item {\it Метод Гаусса}
	
	Приводим матрицу к ступенчатому виду, остались какие-то переменные. Назовем главными те, которые на диагонали, остальные -- свободные. Теперь переносим свободные через =, и присваивая им все возможные значения легко получить решения для главных, а значит для всей системы.
	
	Подробнее \href{https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%B0}{здесь}
		
	\item {\it Метод Гаусса-Жордана}
	
	\begin{enumerate}
		\item Выбирают первый слева столбец матрицы, в котором есть хоть одно отличное от нуля значение.
		\item Если самое верхнее число в этом столбце ноль, то меняют всю первую строку матрицы с другой строкой матрицы, где в этой колонке нет нуля.
		\item Все элементы первой строки делят на верхний элемент выбранного столбца.
		\item Из оставшихся строк вычитают первую строку, умноженную на первый элемент соответствующей строки, с целью получить первым элементом каждой строки (кроме первой) ноль.
		\item Далее проводят такую же процедуру с матрицей, получающейся из исходной матрицы после вычёркивания первой строки и первого столбца.
		\item После повторения этой процедуры $(n-1)$ раз получают верхнюю треугольную матрицу
		\item Вычитают из предпоследней строки последнюю строку, умноженную на соответствующий коэффициент, с тем, чтобы в предпоследней строке осталась только 1 на главной диагонали.
		\item Повторяют предыдущий шаг для последующих строк. В итоге получают единичную матрицу и решение на месте свободного вектора (с ним необходимо проводить все те же преобразования).
		
	\end{enumerate}

	Короче приводим к единичной, что осталось у свободных членов и есть решение
	
	\item {\it Метод Крамера}
	
	Для системы $n$ линейных уравнений с $n$ неизвестными (над произвольным полем)
	
	$\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\ldots +a_{1n}x_{n}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\ldots +a_{2n}x_{n}=b_{2}\\\cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\a_{n1}x_{1}+a_{n2}x_{2}+\ldots +a_{nn}x_{n}=b_{n}\\
	\end{cases}$
	
	с определителем матрицы системы $\Delta$ , отличным от нуля, решение записывается в виде
	
	$ x_{i}={\frac {1}{\Delta }}{\begin{vmatrix}a_{11}&\ldots &a_{1,i-1}&b_{1}&a_{1,i+1}&\ldots &a_{1n}\\a_{21}&\ldots &a_{2,i-1}&b_{2}&a_{2,i+1}&\ldots &a_{2n}\\\ldots &\ldots &\ldots &\ldots &\ldots &\ldots &\ldots \\a_{n-1,1}&\ldots &a_{n-1,i-1}&b_{n-1}&a_{n-1,i+1}&\ldots &a_{n-1,n}\\a_{n1}&\ldots &a_{n,i-1}&b_{n}&a_{n,i+1}&\ldots &a_{nn}\\\end{vmatrix}}$
	(i-ый столбец матрицы системы заменяется столбцом свободных членов).
	
	Подставляем вместо соответствующего столбца свободные члены, считаем определитель, делим на определитель всей матрицы, и получаем $x_i$ соответствующий данному столбцу.
	
	\item {\it Матричный метод}
	
	Есть система вида $AX=B$, тогда решением будет $X=A^{-1}B$
	
	Чтобы работало, нужно чтобы матрица $A$ была невырождена, т.е. чтобы определитель был не равен 0
	
\end{itemize}

Еще есть какие-то итерационные и другие методы, но они звучат и выглядят не очень полезными, но можно посмотреть \href{https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0_%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D1%85_%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9#:~:text=%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0%20%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D1%85%20%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85%20%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9%20(%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F,%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%BC%20%E2%80%94%20%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%BC%20%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC%20%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B9%20%D1%81%D1%82%D0%B5%D0%BF%D0%B5%D0%BD%D0%B8}{тут}

\subsection{Линейные операторы в конечномерном пространстве и их матричноепредставление. Характеристический многочлен, собственные числа и собственные вектора линейного оператора. Сопряженные и самосопряженные операторы.}

\subsubsection{ Линейные операторы}

\D {Пусть $X$ и $Y$ -- линейные пространства над полем $F$. Отображение $\mathcal{A}: X \rightarrow Y$ называется линейным оператором, если $\forall x_1, x_2 \in X, \forall \lambda \in F$
\begin{itemize}
	\item $\mathcal{A}(x_1 + x_2) = \mathcal{A}(x_1) + \mathcal{A}(x_2)$
	\item $\mathcal{A}(\lambda \cdot x_1) = \lambda \cdot \mathcal{A}(x_1)$
\end{itemize}

Линейный оператор $\mathcal{A}: X \rightarrow X$ называется автоморфизмом (или гомоморфизмом)

}

Операторы равны, если переводят элементы первого пространства в одинаковые элементы второго пространства.

\D{Пусть $\mathcal{A}: X \rightarrow Y$

Пусть п.п. $X \leftrightarrow \{e_k\}_{k=1}^{n},\ \dim X = n$

Пусть п.п. $Y \leftrightarrow \{h_k\}_{k=1}^{n},\ \dim Y = m$

$\underset{{1\le k \le n}}{\mathcal{A}e_k} = \sum\limits_{i = 1}^{m} \alpha_k^i \cdot h_i \Rightarrow A = ||\alpha_k^i||,$ где $1 \le i \le m, 1 \le k \le m$

$A = \begin{pmatrix}
	\alpha_1^1 & ... & \alpha_n^1\\
	\alpha_1^2 & ... & \alpha_n^2\\
	... & ... & ...\\
	\alpha_1^n & ... & \alpha_n^n\\
	
\end{pmatrix}$

}

\subsubsection{Характеристический многочлен}

\D {Для данной матрицы $A$, $\chi(\lambda) = \det (A - \lambda E)$, где $E$ -- единичная матрица, является многочленом от $\lambda$, который называется {\bf характеристическим многочленом} матрицы $A$ (видимо можно отождествить матрицу с линейным оператором, тогда будет многочлен для оператора)}

Ценность характеристического многочлена в том, то собственные значения матрицы являются его корнями. Действительно, если уравнение $Av = \lambda v$ имеет ненулевое решение, то $(A - \lambda E)v = 0$, значит матрица $A - \lambda E$ вырождена и ее определитель $\det (A - \lambda E) = \chi(\lambda)$ равен 0

{\bf Свойства}

\begin{itemize}
	\item Для матрицы $n \times n$ характеристический многочлен имеет степень $n$
	\item Все корни характеристического многочлена матрицы являются ее собственными значениями
	\item Теорема Гамильтона-Кэли -- если $\chi(\lambda)$ -- характеристический многочлен матрицы $A$, то $\chi(A) = 0$
	\item Характеристические многочлены подобных матриц совпадают
	\item Характеристический многочлен обратной матрицы $\chi_{A^{-1}}(\lambda) = \frac{(-\lambda)^n}{\det A}\chi_A(1/\lambda)$
	\item Если $A$ и $B$ две матрицы $n \times n$, то $\chi_{AB} = \chi_{BA}$. В частности $tr(AB) = tr(BA), \det(AB) = \det (BA)$
	\item В более общем виде, если $A$ --матрица $m \times n$, а $B$ -- матрица $n \times m$, причем $m < n$, так что $AB$ и $BA$ --квадратные матрицы размеров $m$ и $n$ соответственно, то $\chi_{BA}(\lambda) = \lambda ^{n - m}\chi_{AB}(\lambda)$ 
\end{itemize}


\D{Пусть $L$ -- линейное пространство над полем $K$,

$\mathcal{A}:L \rightarrow L$ -- линейный оператор

{\bf Собственным вектором} линейного оператора $\mathcal{A}$ называется такой ненулевой вектор $x \in L$, что для некоторого $\lambda \in K: \mathcal{A}x = \lambda x$

При этом $\lambda$ называют {\bf собственным числом} оператора $\mathcal{A}$

}

{\bf Свойства}

\begin{itemize}
	\item Собственные векторы, отвечающие различным собственным значениям, образуют ЛНЗ набор
	\item Еще какие-то леммы есть, подробнее см на \href{https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%8B_%D0%B8_%D1%81%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5_%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F}{говне}
\end{itemize}

\subsubsection{Сопряженные и самосопряженные операторы}

\D{Пусть $E, L$ -- линейные пространства, а $E^*, L^*$ -- сопряженные линейные пространства (пространства линейных функционалов, определенных на $E$ и $L$). Тогда для любого линейного оператора $\mathcal{A}: E \rightarrow L$ и любого линейного функционала $g \in L^*$ определен линейный функционал $F \in E^*$ -- суперпозиция $g$ и $A: f(x) = g(A(x))$. Отображение $g \rightarrow f$ называется сопряженным линейным оператором и обозначается $\mathcal{A^*}: L^* \rightarrow E^*$. Если кратко, то $(\mathcal{A^*}g, x) = (g, \mathcal{A}x)$
	
Если же $\mathcal{A^*} = \mathcal{A}$, то такой оператор называется самосопряженным, для него $(\mathcal{A}x, y) = (x, \mathcal{A}y)$
}

\subsection{Предмет и основные определения теории вероятностей.Классическое определение вероятности.Зависимые и независимые события. Условные и безусловные вероятности. Формула полной вероятности. Формула Байеса.}

\subsubsection{Теория вероятностей}

Классическое определение вероятности -- есть пространство событий, тогда вероятность события $A$ -- это отношение числа $m$ благоприятных этому событию случаев, к числу всех возможных случаев, т.е. $P(A) = \frac{m}{n}$

\D{ Два события $A$ и $B$ называются {\bf независимыми}, если $p(A \cap B) = p(A) \cdot p(B)$}

\D {Два события $A$ и $B$ называются {\bf несовместными}, если $A \cap B = \emptyset$}

\D{ {\bf Условная вероятность}: Пусть задано вероятностное пространство $(\Omega, P)$. Условной вероятностью события $A$ при условии, что произошло событие $B$, называется число 
	
$P(A | B) = \frac{P(A \cap B)}{P(B)}$, где $A, B \in \Omega$}

Если $P(B) = 0$, то определение неприменимо.

\subsubsection{Формула полной вероятности}

\D {Вероятность события $A \in \Omega$, которое может произойти только вмест с одним из событий $B_1, B_2, ..., B_n$, образующих полную систему событий, равна сумме произведений вероятностей гипотез на условные вероятности события, вычисленные соотвественно при каждой из гипотез

$P(A) = \sum\limits_{i=1}^n P(A | B_i)P(B_i)$
}

При этом полная система -- когда $B_i \cap B_j = \emptyset$, а $P(B_i) > 0, B_1 \cup ... \cup B_n = \Omega$

\subsubsection{Формула Байеса}

\D {Формула Байеса -- соотношение различных предполагаемых вероятностей различных событий, которое дает вероятность, что какое-то событие $A$ является результатом $X$ ряда независимых друг от друга событий $B_1, B_2, ..., B_n$, который, возможно, привел к $A$

$P(B_i|A) = \frac{P(A | B_i)P(B_i)}{P(A)}$

}

\subsection{Математическое ожидание случайной величины. Его смысл и примеры. Свойства математического ожидания. Дисперсия и среднее квадратическое отклонение случайной величины. Их смысл и примеры вычисления. Формулы для вычисления дисперсии. Свойства дисперсии}

\subsubsection{Математическое ожидание}

\D{ Пусть задано вероятностное пространство $(\Omega, \mathbb{P})$ и определенная на нем соучайная величина $X$, т.е. по определению $X: \Omega, \mathbb{R}$ -- измеримая функция. Если существует интеграл Лебега от $X$ по пространству $\Omega$, то он и называется матожиданием

$\mathbb{E}[X] = \int\limits_{\Omega} X(\omega)\mathbb{P}(d \omega)$

В дискретном случае выглядит как $\sum X(\omega) p(\omega)$
}

Смысл -- ожидаемое среднее значение если взять много сэмплов случайной величины и усреднить.

Пример: честная кость -- $E[X] = \frac{1}{6} \cdot 1 + ... + \frac{1}{6} \cdot 6 = 3.5$

{\bf Свойства}

\begin{itemize}
	\item $E(a) = a,$ где $a$ -- константа
	\item Если $0 \le \xi \le \eta$ и $\eta$ -- случайная величина с конечным матожиданием, то $0 \le E(\xi) \le E(\eta)$
	\item $\xi = \eta \Rightarrow E(\xi) = E(\eta)$
	\item Если $\xi$ и $\eta$ независимы, то $E(\xi \cdot \eta) = E(\xi) \cdot E(\eta)$
	\item Матожидание линейно, т.е. $E(aX + bY) = aE(X) + bE(Y)$
	
\end{itemize}

\subsubsection{Дисперсия}

\D {Дисперсией случайной величины называется матожидание квадрата отклонения этой величины от ее матоидания -- $DX = E(X - EX)^2$}

Можно по линейности переписать как $DX = EX^2 - (EX)^2$

{\bf Среднеквадратическое отклонение -- корень дисперсии $\sigma = \sqrt{DX}$}

Дисперсия характеризует разброс случайной величины вокруг ее матожидания. Среднеквадратичное отклонение используется для оценки масштаба возможного отклонения

{\bf Свойства}

\begin{itemize}
	\item Дисперсия неотрицательна
	\item Если дисперсия конечна, то конечно и матожидание
	\item Дисперсия константы равна 0 $Da = 0$, также верно обратное $DX = 0 \Rightarrow X = EX$ почти всюду
	\item $D(X + Y) = DX + DY + 2cov(X, Y)$, где $cov(X, Y) = E[(X - EX)(Y - EY)]$
	\item Для независимых величин ковариация равна 0
	\item $D(aX) = a^2DX$
	\item $D(-X) = DX$
	\item $D(X+b) = DX$
\end{itemize}


\subsection{ Дискретные случайные величины. Математическое ожидание и дисперсия. Стандартные дискретные распределения (Бернулли, биномиальное, геометрическое, Пуассона).}

\D { Случайная величина -- отображение из множества элементарных исходов в множество вещественных чисел}

\D { {\bf Дискретной случайной величиной} называется случайная величина, множество значений которой не более чем счетно, причем принятие ею каждого из значений есть случайное событие с определенной вероятностью}

\D {{\bf Функция распределения} случайной величины -- функция $F(x)$, определенная на $\mathbb{R}$ как $P(\xi \le x)$ т.е. выражаюшая вероятность того, что $\xi$ примет значение меньшее или равное $x$}

Матожидание дискретной величины -- $E\xi = \sum \xi_i p(\xi_i)$, где $\xi_i$ -- возможный исход, а $p_{\xi_i}$ -- его вероятность.

Дисперсия пересчитывается по формуле $DX = EX^2 - (EX)^2$

\subsubsection{ Стандартные дискретные распределения}

{\bf Распределение Бернулли}

Величина принимает всего 2 значения -- $1$ и $0$ с вероятностями $p$ и $q = 1 - p$ соответственно

{\bf Биноминальное распределение}

Это распределение количества "успехов" \ в последовательности из $n$ независимых случайных экспериментов, таких, что вероятность успеха в них одинакова и равна $p$.

Т.е. есть последовательность $X_1, ..., X_n$ -- независимых случайных величин имеющих распределение Бернулли с параметром $p$, тогда $Y = X_1 + ... + X_n$ имеет биноминальное распределени с параметрами $n$ и $p = Bin(n, p)$ 

В этом случае функция вероятности задается формулой $\mathbb{P}(Y = k) = {n \choose k} p^k q^{n - k}, k = 0, ..., n$

${n \choose k} = C_n^k = \frac{n!}{k!(n-k)!}$

{\bf Геометрическое распределение}

Одно из двух

\begin{enumerate}
	\item распределение вероятностей случайной величины $X$ равой номеру первого успеха в серии испытаний Бернулли и принимающей значение $n = 1, 2, 3,...$
	\item Распределение вероятностей случайной величины $Y = X - 1$ равной числу неудач до первого успеха, и принимающей значения $n = 0, 1, 2,...$
\end{enumerate}

В cлучае первого успеха $P(X = n) = (1 - p)^{n - 1}p$

В случае количества неудач -- $P(Y = n) = (1 - p)^n p$


{\bf Распределение Пуассона}

Распределение числа событий, произошедших за фиксированое время, при условии что они происходят с некоторой фиксированной средней интенсивностью и независимо друг от друга

Выберем фиксированное $\lambda > 0$ и определим дискретное распределение по функции вероятности:

$p(k) = P(Y = k) = \frac{\lambda^k}{k!}e^{-\lambda}$

где 

$k$ -- количество событий

$\lambda$ -- матожидание величины 



\subsection{Непрерывные случайные величины и их функции распределения. Математическое ожидание и дисперсия. Стандартные непрерывные распределения (равномерное, показательное, нормальное).}

\subsubsection{Непрерывные случайные величины}

\D { Непрерывной случайной величиной называют случайную величину, которая в результате испытания принимает все значения из некоторого числового промежутка. Число возможных значений непрерывной случайной величины бесконечно.}

$F_X(x) = P(X \le x)$


В непрерывном случае $\P(X = x) = 0 \forall x \in \mathbb{R}$
 и $F_X(x - 0) = F_X(x), \forall x \in \mathbb{R}$
 
А следовательно формулы имеют вид $P(X \in |a, b|) = F_X(b) - F_X(a)$



Матожидание для непрерывных величин считается по формуле $EX = \int\limits_{-\infty}^{+\infty}xf(x)dx$, где $f(x)$ -- функция плотности распределения (штука от которой интеграл равен 1, говорит с какой вероятностью выпадет то или иное значение (или отношение для разных значений в непрерывном случае))


Дисперсия же вычисляется по формуле $DX = \int\limits_{-\infty}^{+\infty} x^2 f(x) dx - (EX)^2$

\subsubsection{Стандартные равномерные распределения}

{\bf Равномерное распределение}

Распределение, которое принимает значения из некоторого промежутка конечной длины, при этом плотность вероятности на этом промежутке почти всюду постоянна.

Т.е. с равной вероятностью может выпасть любое значение из промежутка.

Плотность имеет вид $f_X(x) = \begin{cases}
\frac{1}{b - a}, x \in [a, b]\\
0, x \not\in [a, b]
\end{cases}$

{\bf Показательное (экспоненциальное) распределение}

Случайная величина имеет экспоненциальное распределение с параметром $\lambda > 0$, если ее плотность вероятности имеет вид

$f_X(x) = \begin{cases}
\lambda e^{-\lambda x}, x \ge 0\\
0, x < 0
\end{cases}$

{\bf Нормальное распределение}


Плотность вероятности $f(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}$

где $\mu$ -- матожидание

$\sigma$ -- среднеквадратическоее отклонение

Данное распределение моделирует ситуацию когда есть какое-то среднее и какая-то дисперсия, Наибольшая вероятность получить среднее, в зависимости от дисперсии получаем значения дальше или ближе к среднему.


\subsection{Вероятностные неравенства Йенсена, Маркова и Чебышёва. Правило трёх сигм. Закон больших чисел}

\subsubsection{Неравенство Йенсена}

\D {Пусть $(\Omega, \mathcal{F}, \mathbb{P})$ -- вероятностное пространство, и $X: \Omega \rightarrow \mathbb{R}$ -- определенная на нем случайная величина. Пусть также $\phi: \mathbb{R} \rightarrow \mathbb{R}$ -- выпуклая (вниз) борелевская функция. Тогда если $X, \phi(X) \in L^1 (\Omega, \mathcal{F}, \mathbb{P})$, то
$$\phi(EX) \le E(\phi(X))$$. Также можно добавить условность}

\subsubsection{Неравенство Маркова}

\D {Пусть неотрицательная случайная величина $X: \Omega \rightarrow \mathbb{R}^{+}$ определена на вероятностном пространстве, и ее матожидание конечно. Тогда 

$\mathbb{P}(X \ge a) \le \frac{EX}{a}$}

{\bf Док-во:}

Пусть неотрицательная лучайная величина $X$ имеет плотность распределения $p(x)$, тогда для $a > 0$

$EX = \int\limits_{0}^{\infty} xp(x)dx \ge \int\limits_{a}^{\infty}xp(x)dx \ge \int\limits_{a}^{\infty}ap(x)dx = a \mathbb{P}(X \ge a)$


\subsubsection{Неравенство Чебышева}

\D {Пусть случайная величина $X$ определена на вероятностном пространстве, а ее матожидание $\mu$ и дисперсия $\sigma^2$ конечны. Тогда

$P(|X - \mu| \ge a) \le \frac{\sigma^2}{a^2}, a > 0$

Если $a = k\sigma$, где $\sigma$ -- стандартное отклонение, $k > 0$, то получаем

$P(|X - \mu| >\ge k\sigma) \le \frac{1}{k^2}$
}


\subsubsection{Правило трех сигм}

Если случайная величина распределена нормально, то абсолютная величина ее отклонения от матожидания не превосходит утроенного среднего квадратического отклонения

$P(|X - \mu| \ge 3 \sigma) \le \frac{1}{9}$


\subsubsection{Закон больших чисел}

\D {Рассмотрим последовательность независимых в совокупности случайных величин $X_1, X_2, ...$ интегрируемых по Лебегу, которые имеют одинаковые распределения, следовательно, и одинаковые матожидания. Обозначим через $\overline{X}_n$ среднее арифметическое рассматриваемых случайных величин. Оно сходится к матожиданию}

{\bf Слабый закон}

Сходится по вероятности к матожиданию 

$\overline{X}_n \rightarrow \mu$ при $n \rightarrow 0$

\subsection{ Статистические оценки параметров распределения (сущность теории оценивания): несмещенность, состоятельность, эффективность оценок. Точечные оценки: выборочная средняя, дисперсия, среднее квадратическое отклонение.}


\subsubsection{Статистические оценки}

\D {Статистическая оценка -- то статисткика, которая используется для оценивания неизвестных параметров распределений сслучайной величины. Задача статистической оценки формулируется так

Пусть $\xi = (\xi_1, ..., \xi_n)$ -- выборка из генеральной совокупности с распределением $F_{\xi}(x, \theta)$. Распределение $F_{\xi}$ имеет известную функциональную форму, но зависит от неизвестного параметра $\theta$/ Этот параметр может быть любой точкой заданноко параметрического множества $\Theta$. Испольщуя статистическую информацию, содержащуюся в выборке $\xi$, сделать выводы о настоящем параметре значения $\theta$ }

Например посчитать матожидание как среднее арифметическое в предположении о нормальности.

Свойства оценок

\begin{itemize}
	\item Несмещенность -- если матожидание оценки совпадает с оцениваемым параметром $E\theta = \theta$
	\item Состоятельность -- При увеличении числа опытов оценка $\hat{\theta}$ сходится по вероятности к параметру $\theta$
	\item Эффективность -- Если дисперсия несмещенной оценки $D\hat{\theta}$ является минимальной по сравнению с другими оценками 
\end{itemize}

\subsubsection{Точечная оценка}

\D {Пусть $X_{1},\ldots ,X_{n},\ldots$  — случайная выборка для распределения, зависящего от параметра $\theta \in \Theta$ . Тогда статистику $\hat {\theta }(X_{1},\ldots ,X_{n})$, принимающую значения в $\Theta$, называют точечной оценкой параметра $\theta$ .}

\begin{itemize}
	\item Выборочное среднее -- Пусть $X_1, ..., X_n$ -- выборка из распределения вероятности, определеннная на некотором вероятностном пространстве. Тогда ее выборочным средним называется случайная величина $\overline{X} = \frac{1}{n}\sum X_i$
	
	Свойства: 
	\begin{itemize}
		\item ВЫборочное среднее -- несмещенная оценка теоретического среднего
		
		$E\overline{X} = E X_i, i = 1, ..., n$
		\item Выборочное среднее -- сильно состоятельная оценка теоретического среднего
		
		$\overline{X} \rightarrow E X_i $почти наверное при $n \rightarrow \infty$
		
		\item Выборочное среднее -- асимптотически нормальная оценка.
		
		Пусть дисперсия случайных величин $X_{i}$ конечна и ненулевая, то есть $\mathrm {D} [X_{i}]=\sigma ^{2}<\infty ,\sigma ^{2}\not =0,\;i=1,\ldots ,n$. Тогда
	    $\sqrt {n}\left({\overline {X}}-\mathbb {E} [X_{1}]\right)\to \mathrm {N} (0,\sigma ^{2})$ по распределению при $n\to \infty$ ,
		где $\mathrm {N} (0,\sigma ^{2})$ — нормальное распределение со средним $0$ и дисперсией $\sigma ^{2}$.
	\end{itemize}
	
	\item Выборочная дисперсия
	
	Пусть $X_{1},\ldots ,X_{n},\ldots$  — выборка из распределения вероятности. Тогда
	
	выборочная дисперсия — это случайная величина
	
	$S_{n}^{2}={\frac  {1}{n}}\sum \limits _{{i=1}}^{n}\left(X_{i}-{\bar  {X}}\right)^{2}={\frac  {1}{n}}\sum \limits _{{i=1}}^{n}X_{i}^{2}-\left({\frac  {1}{n}}\sum \limits _{{i=1}}^{n}X_{i}\right)^{2}$,
	
	где символ $\bar  {X}$ обозначает выборочное среднее;
	
	несмещённая (исправленная) дисперсия — это случайная величина
	
	$S^{2}={\frac  {1}{n-1}}\sum \limits _{{i=1}}^{n}\left(X_{i}-{\bar  {X}}\right)^{2}$.
	
	\item Среднеквадратическое отклонение 
	
	$\sum _{i=1}^{n}\left(x_{i}-{\bar {x}}\right)^{2}.$
\end{itemize}

\subsection{Интервальные оценки. Точность оценки. Доверительная вероятность. Доверительные интервалы для оценки неизвестного значения генеральной средней и генеральной доли.}

\subsubsection{Интервальная оценка}

\D {Пусть $ X=(X_{1},\ldots ,X_{n})$ — случайная выборка объёма $n$, порождённая случайной величиной с функцией распределения вероятностей $ F(x;\theta )$, известной с точностью до параметра $\theta \in \Theta$ . Располагая выборкой $X$, необходимо найти оценку $\hat {\theta }$ параметра $\theta \in \Theta$ . В общем случае имеется нулевая вероятность того, что $\hat {\theta }=\theta$ — что точечная оценка $\hat {\theta }$ совпадёт с параметром $\theta$ . Поэтому для оценивания параметра используется интервальная оценка.
	
Проблема состоит в нахождении на основании выборки статистик $\hat {\theta _{1}}={\hat {\theta _{1}}}(X_{1},\ldots ,X_{n})$, $ {\hat {\theta _{2}}}={\hat {\theta _{2}}}(X_{1},\ldots ,X_{n})$, которые с достоверностью удовлетворяют неравенству ${\hat {\theta _{1}}}<\theta <{\hat {\theta _{2}}}$. Зададимся достаточно малым числом $\alpha$  — уровнем значимости. Тогда интервал $ [{\hat {\theta _{1}}},{\hat {\theta _{2}}}]$ называется интервальной оценкой параметра $\theta $, если $ P({\hat {\theta _{1}}}<\theta <{\hat {\theta _{2}}})=1-\alpha $.
	
Интервал $ I(X)=[{\hat {\theta _{1}}}(X),{\hat {\theta _{2}}}(X)]$ называется доверительным интервалом параметра на уровне значимости $\alpha$ или с надежностью $1-\alpha$ .}

{\bf Точность оценки} -- положительное число $\delta > 0$ характеризующее величину расхождения оценками выборки и генеральной совокупности

$|\theta - \hat{\theta}| < \delta$

{\bf Надежностью (доверительной вероятностью)} оценки $\theta$ по $\hat{\theta}$ называют вероятность $\gamma$, с которой осуществляется неравенство $|\theta - \hat{\theta}| < \delta$. Обычно задается наперед, часто берут наджность близкую к единице.

Доверительные интервалы для генеральной средней  и доли \href{http://mathprofi.ru/ocenki_po_povtornoy_i_bespovtornoy_vyborke.html}{тут}




\section{Машинное обучение}

\subsection{Понятие машинного обучения в искусственном интеллекте. Классификация задач
машинного обучения, их примеры и особенности.}

\D{
    Машинное обучение - класс методов искусственного интеллекта,
    характерной чертой которых является не прямое решение задачи,
    а обучение за счёт применения решений множества сходных задач.
}

По имеющейся совокупности прецедентов (пар "объект" - "ответ") требуется
восстановить неявную зависимость, построить алгоритм, способный для любого
входного объекта выдать достаточно точный ответ.

Классификация задач машинного обучения:
\begin{itemize}
    \item \textbf{Обучение с учителем} - для каждого прецедента задается пара "ситуация, требуемое решение"
    \item \textbf{Обучение без учителя} - для каждого прецедента задаётся только
    «ситуация», требуется сгруппировать объекты в кластеры, используя данные
    о попарном сходстве объектов, и/или понизить размерность данных.
    \item \textbf{Обучение с подкреплением} - способ машинного обучения, при котором
    система обучается, взаимодействуя с некоторой средой.
    \item \textbf{Semi-supervised learning} - для части примеров известны ответы, а для части нет
\end{itemize}

Классические задачи машинного обучения:
\begin{itemize}
    \item Классификация
    \item Кластеризация
    \item Регрессия
    \item Понижение размерности
    \item Восстановление плотности распределения вероятности
\end{itemize}

\subsection{Задача обучения с учителем, классификация задач обучения с учителем, метод
минимизации эмпирического риска.}

Пусть $X$ - множество описаний объектов, $Y$ - множество допустимых ответов.
Существует неизвестная целевая зависимость $y^*: X \to Y$, значения которой
известны только на объектах конечной обучающей выборки $X^m = \{(x_1, y_1)\dots (x_m, y_m)\}$.

Требуется построить алгоритм $a: X \to Y$, который приближал бы неизвестную
зависимость как на элементах выборки, так и на всем множестве $X$.

Типы задач обучения с учителем:
\begin{itemize}
    \item Классификация = $|Y| < \infty$
    \item Регрессия = $|Y| = \infty$
\end{itemize}

Вводится функция потерь $\mathcal{L}(y, y')$, характеризующая величину
отклонения ответа $y = a(x)$ от правильного ответа $y' = y^*(x)$ на
произвольном объекте $x \in X$.

Вводится функционал качества, характеризующий среднюю ошибку
(эмпирический риск) алгоритма $a$ на произвольной выборке $X^m$:

$Q(a, X^m) = \frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(a(x_i), y^*(x_i))$

\textbf{Метод минимизации эмпирического риска} заключается в том, чтобы
в заданной модели алгоритмов $A = \{a: X \to Y\}$ найти алгоритм, минимизирующий
среднюю ошибку на обучающей выборке:

$a^* = arg \min\limits_{a \in A} Q(a, X^m)$

Тем самым задача сводится к оптимизации и может быть решена численными методами.


\subsection{Понятие переобучения и методы борьбы с ним.}

Обобщающая способность. Говорят, что алгоритм обучения обладает способностью
к обобщению, если вероятность ошибки на тестовой выборке достаточно
мала или хотя бы предсказуема, то есть не сильно отличается от ошибки
на обучающей выборке.

\D {
	Переобучение (англ. overfitting) — негативное явление, возникающее,
	когда алгоритм обучения вырабатывает предсказания, которые слишком
	близко или точно соответствуют конкретному набору данных и поэтому
	не подходят для применения алгоритма к дополнительным данным или
	будущим наблюдениям.
}

Методы борьбы с переобучением
\begin{itemize}
	\item Увеличение количества данных в наборе
	\item Уменьшение количества параметров модели
	\item Добавление регуляризации
\end{itemize}


\subsection{Меры оценки качества для классификации и регрессии.}

\subsubsection{Оценка качества классификации}

\begin{table}[h!]
	\begin{tabular}{|c|c|c|}
		\hline
		& $y = 1$ & $y = 0$ \\
		\hline
		$a(x) = 1$ & TP & FP \\
		\hline
		$a(x) = 0$ & FN & TN \\
		\hline
	\end{tabular}
	\caption{Confusion matrix}
\end{table}

\D{
	\textbf{Accuracy} = доля верных ответов алгоритма.

	$Acc = \frac{TP + TN}{TP + FP + TN + FN}$
}

\D{
	\textbf{Precision} = доля объектов действительно принадлежащих данному
	классу среди всех объектов, отнесенных моделью к данному классу.

	$Precision = \frac{TP}{TP + FP}$

	По confusion matrix: отношение диагонального элемента к сумме строки.
}

\D{
	\textbf{Recall} = какую долю объектов, реально относящихся к положительному
	классу мы предсказали верно.

	$Recall = \frac{TP}{TP + FN}$

	По confusion matrix: отношение диагонального элемента к сумме столбца.
}

\D{
	$F = \frac{2 \times Precision \times Recall}{Precision + Recall}$
}

\D{
	\textbf{ROC-curve} = Позволяет рассмотреть все пороговые значения для
	данного классификатора. Показывает долю ложно положительных примеров
	(англ. false positive rate, FPR) в сравнении с долей истинно положительных
	примеров (англ. true positive rate, TPR). 

	Может давать неадекватную оценку при несбалансированности классов.
}

$TPR = Recall = \frac{TP}{TP + FN}$

$FPR = \frac{FP}{FP + TN} = 1 - TNR$, $TNR = specificity$

Прямая диагональная линия соответствует чисто случайному классификатору.
Хороший классификатор характеризуется большей площадью под кривой.

\D{
	\textbf{PR-curve} = x - полнота, y - точность.
}

\subsubsection{Оценка качества регрессии}

\begin{itemize}
	\item $MSE = \frac{1}{n} \sum\limits_1^n (a(x_i) - y_i)^2$ - позволяет сравнивать модели, но
	не позволяет определить, насколько хорошо алгоритм решает задачу.
	\item $MAE = \frac{1}{n} \sum\limits_1^n |a(x_i) - y_i|$ - менее чувствителен к выбросам.
	\item $R^2 = 1 - \frac{\sum\limits_1^n (a(x_i) - y_i)^2}{\sum\limits_1^n (y_i - \overline{y})^2}$ -
	Коэффициент детерминации измеряет долю дисперсии, объясненную моделью, в общей дисперсии целевой
	переменной. Фактически, данная мера качества — это нормированная среднеквадратичная ошибка. Если
	она близка к единице, то модель хорошо объясняет данные, если же она близка к нулю, то прогнозы
	сопоставимы по качеству с константным предсказанием.
	\item $MAPE = 100\% \times \frac{1}{n}\sum\limits_1^n \frac{|y_i - a(x_i)|}{|y_i|}$ - процент
	который составляет ошибка от фактических значений.
	\item $RMSE = \sqrt{\frac{1}{n} \sum\limits_1^n (a(x_i) - y_i)^2}$
\end{itemize}


\subsection{Метод ближайших соседей. Метод ядерного сглаживания (непараметрическая
регрессия)}

\subsubsection{Метод ближайших соседей}

\D{
	\textbf{Метрический классификатор} = алгоритм классификации основанный на вычислении
	оценок сходства между объектами.

	\textbf{kNN} = классифицируемый объект относится к тому классу, к которому
	принадлежит большинство его соседей (k ближайших элементов обучающей выборки).
}

Пусть на множестве объектов задана функция расстояния $\rho(x, x')$, тогда
любой новый объект $u$ задает порядок на обучающей выборке
$\rho(u, x_{1; u}) \leq \rho(u, x_{2; u}) \leq ... \leq \rho(u, x_{m; u})$

Алгоритм ближайших соседей: $a(u) = argmax_{y \in Y} \sum\limits_{i=1}^m
[y_{i;u} = y] w(i, u)$, где $w(i, u)$ - весовая функция, которая определяет
степень важности i-того соседа для классификации объекта $u$.

\begin{itemize}
	\item $w(i, u) = [i = 1]$ - метод 1 ближайшего соседа.
	\item $w(i, u) = [i \leq k]$ - метод k ближайших соседей.
	\item $w(i, u) = [i \leq k]q^i$ - метод k экспоненциально взвешенных ближайших соседей, $q < 1$
	\item $w(i, u) = K\left(\frac{\rho(u, x_{i; u})}{h}\right)$ - метод парзеновского окна фиксированной ширины h.
	\item $w(i, u) = K\left(\frac{\rho(u, x_{i; u})}{\rho(u, x_{k+1; u})}\right)$ -
	метод парзеновского окна переменной ширины.
	\item $w(i, u) = K\left(\frac{\rho(u, x_{i; u})}{h(x_{i; u})}\right)$ -
	метод потенциальных функций, в котором ширина окна $h(x_i)$ зависит не от
	классифицируемого объекта, а от обучающего объекта $x_i$.
\end{itemize}

Здесь $K(r)$ - ядро сглаживания = неотрицательная, монотонно невозрастающая функция на $[0, +\infty)$

Примеры ядер:
\begin{itemize}
	\item $1 - |r|$
	\item $\frac{3}{4}(1 - r^2)$
	\item $\frac{70}{81} (1 - |r|^3)^3$
\end{itemize}

\subsubsection{Метод ядерного сглаживания}

\textbf{Ядро} = непрерывная ограниченная симметричная вещественная функция
$K$, т.ч. $\int K (u) du = 1$

Представляет собой построение весовой функции $W_{mi} (x)$, определенной
на всех элементах обучающей выборки. Затем эти веса используются для
представления $a(x)$ в виде взвешенной суммы $y_i$.

$a(x) = argmax_{y \in Y} \sum [y_i = y] W_{mi} (x_i)$

Короче то же самое, что и кнн с ядрами.


\subsection{Метод градиентного спуска, усовершенствования метода градиентного
спуска.Линейная регрессия. Сингулярное разложение векторов.}

\subsubsection{Градиентный спуск}

Решает задачу поиска минимума функции $f: \mathbb{R}^n \to \mathbb{R}$ для
функций, у которых можно вычислить градиент.

Основная идея метода заключается в том, чтобы осуществлять оптимизацию
в направлении скорейшего спуска, которое задается антиградиентом $-\nabla f$.

$x_{k+1} = x_k - \lambda_k \nabla f(x_k)$

Критерии останова:
\begin{itemize}
	\item $||x_{k+1} - x_{k}|| \leq \varepsilon$
	\item $||f(x_{k+1}) - f(x_k)|| \leq \varepsilon$
\end{itemize}

Выбор LR = $\lambda_k$:
\begin{itemize}
	\item $\lambda_k = const$
	\item Пока условие $f(x_{k+1}) = f(x_k - \lambda_k f'(x_k)) \leq
	f(x_k) - \varepsilon\lambda_k ||f'(x_k)||^2$ выполняется $\lambda_k$
	остается константным, иначе $\lambda_{k+1} = \lambda_k \cdot \delta$
	- метод дробного шага.
	\item Метод наискорейшего спуска: $\lambda_k = arg 
	\min\limits_{\lambda \in [0, +\infty)} f(x_k - \lambda f'(x_k))$
\end{itemize}

\subsubsection{Линейная регрессия}

\D{
	\textbf{Линейная регрессия} - метод восстановления зависимости
	одной переменной от других с линейной функцией зависимости.
}

Для заданного множества пар $\{(x_i, y_i)\}$ значений свободной и зависимой
переменной требуется построить зависимость.

Назначена линейная модель с аддитивной случайной величиной $\varepsilon$:

$y_i = f(w, x_i) + \varepsilon_i$

Предполагается, что $\varepsilon$ распределена нормально с нулевым матожиданием
и фиксированной дисперсией $\sigma^2$, не зависящей от $x, y$. При таких
предположениях параметры $w$ регрессионной модели вычисляются методом наименьших
квадратов.

\begin{itemize}
	\item $f_1(x), ..., f_n(x)$ - числовые признаки объектов.
	\item $f(w, x) = \sum\limits_1^n w_j f_j(x)$ - линейная модель.
\end{itemize}

\begin{equation}
	F =
	\begin{pmatrix}
		f_1(x_1) & ... & f_n(x_1) \\
		... & ... & ... \\
		f_1(x_m) & ... & f_n(x_m)
	\end{pmatrix},
	y = 
	\begin{pmatrix}
		y_1 \\
		\vdots \\
		y_m
	\end{pmatrix},
	w =
	\begin{pmatrix}
		w_1 \\
		\vdots \\
		w_n
	\end{pmatrix}
\end{equation}

МНК:

$Q(w, X^m) = \sum\limits_{i=1}^m (f(w, x_i) - y_i)^2 = ||F w - y||^2 \to_w \min$

$\frac{d Q}{d w} = 2F^T(F w - y) = 0$ - условие минимума.

$\Rightarrow F^T F w = F^T y \Rightarrow w^* = (F^T F)^{-1}F^T y = F^+ y$

\subsubsection{Сингулярное разложение}

\T{
	У любой матрицы $A$ размера $n \times m$ существует разложение на
	матрицы $U, \Sigma, V^T: A_{n \times m} = U_{n \times n} \times \Sigma_{n \times m} \times V^T_{m \times m}$,
	где матрицы $U, V$ являются ортогональными ($U U^T = U^T U = E$),
	а матрица $\Sigma$ - диагональной.
}

Столбцы $U$ - собственные векторы матрицы $F F^T$, столбцы $V$ - собственные
векторы матрицы $F^T F$.

$F = VDU^T$

$F^+ = (U D V^T V D U^T)^{-1} U D V^T = U D^{-1} V^T = \sum\limits_1^n
\frac{1}{\sqrt{\lambda_j}} u_j v_j^T$

$w^* = \sum\limits_1^n \frac{1}{\sqrt{\lambda_j}} u_j (v_j^T y)$


\subsection{Метод опорных векторов для линейно разделимой выборки. Ядра и ядерный
трюк в методе опорных векторов.}

\href{https://neerc.ifmo.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2_(SVM)}{SVM}


\subsection{Вероятностная постановка задачи классификации и оптимальный
байесовский классификатор.Непараметрический метод оценки плотности.}

\href{
	https://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F
}{Вероятностная постановка + опт байес}

\href{
	http://www.machinelearning.ru/wiki/index.php?title=%D0%9E%D1%86%D0%B5%D0%BD%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BF%D0%BB%D0%BE%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F
}{
	Метод оценки плотности (непарам)
}


\subsection{Параметрический метод оценки плотности и метод максимального
правдоподобия.}

\href{
	http://www.machinelearning.ru/wiki/index.php?title=%D0%9E%D1%86%D0%B5%D0%BD%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BF%D0%BB%D0%BE%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F
}{
	Метод оценки плотности и метод макс правдоподобия
}

\subsection{Логистическая регрессия.}

\href{http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F}{
	Logreg
}


\subsection{Логические правила и вывод закономерностей. Деревья принятия решений.}


\subsection{Задача бустинга и градиентный бустинг. AdaBoost и его теоретические свойства.}

\href{
	https://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3,_AdaBoost
}{
	Бустинг, adaboost
}

\href{
	https://logic.pdmi.ras.ru/~sergey/teaching/mlstc12/16-boosting.pdf
}{
	Теоретические свойства AdaBoost
}


\subsection{Случайный лес и стэкинг.}

\D{
	\textbf{Стэкинг} - метод ансамблирования моделей, при котором учитываются
	ответы всех моделей (например как среднее или majority vote)
}

При стекинге обучающую выборку делят на 2: на первой обучают базовые алгоритмы,
на второй получают мета-признаки из базовых алгоритмов и обучают метаалгоритм.

\href{
	https://learnmachinelearning.fandom.com/ru/wiki/%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9#%D0%A1%D1%82%D1%8D%D0%BA%D0%B8%D0%BD%D0%B3_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9
}{Стэкинг}

\href{
	https://neerc.ifmo.ru/wiki/index.php?title=%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9_%D0%B8_%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BB%D0%B5%D1%81#.D0.A1.D0.BB.D1.83.D1.87.D0.B0.D0.B9.D0.BD.D1.8B.D0.B9_.D0.BB.D0.B5.D1.81
}{Случайный лес}


\subsection{Нейрон Маккалока-Питтса, выразительная мощность нейрона. Многослойная
нейронная сеть и алгоритм обратного распространения ошибок.}


\end{document}
