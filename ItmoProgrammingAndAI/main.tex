%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
% \documentclass[14pt]{extarticle}
\usepackage{pdfpages}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Programming and AI} % Title of the assignment

%----------------------------------------------------------------------------------------

\DeclareMathOperator*\uplim{\overline{lim}}

\usepackage[parfill]{parskip}
\usepackage{listings}

\begin{document}
	
\maketitle % Print the title
\section{Математика и Теоретическая информатика}

\subsection{Числовые ряды. Абсолютная и условная сходимость. Признаки сходимости числовых рядов.}

\subsubsection{Числовые ряды}

$\sum\limits_{k=1}^{\infty} a_{k} = a_{1} + a_{2} + a_{3} + \cdots$ -- числовой ряд

Сходимость ряда означает существование конечной суммы, т.е. $\sum\limits_{k=1}^{\infty} a_{k} = S$ где $S$ -- конечное число, иначе ряд считается расходящимся.

\subsubsection{Абсолютная и условная сходимость}

Ряд $\sum\limits_{k=1}^{\infty} a_{k}$ называется {\bf абсолютно} сходящимся, если сходится ряд из модулей $\sum\limits_{k=1}^{\infty} |a_{k}|$, иначе ряд называется {\bf условно} сходящимся

\subsubsection{Признаки сходимости числовых рядов} 

{\bf Знакоположительные ряды} (ряды с положительными членами):

Критерий сходимости знакоположительных рядов-- знакоположительный ряд $\sum\limits_{k=1}^{\infty} a_{k}$ сходится тогда и только тогда, когда последовательность его частичных сумм $S(n) = \sum\limits_{k=1}^{k=n}a_{k}$ ограничена сверху

{\bf Док-во:}

=>: ряд сходится, значит последовательность частичных сумм $\S(n) =\sum\limits_{k=1}^{n} a_{k}$ имеет предел равный $\sum\limits_{k=1}^{\infty} a_{k} = S$

<=: Пусть дан положительный ряд и последовательность частичных сумм ограничена сверху, заметим что последовательность частичных сумм неубывающая:
$$S_{n + 1} - S_{n} = a_{n + 1} \ge 0$$. Используя свойство из теоремы о монотонной последовательности получаем, что т.к. последовательность частичных сумм монотонно не убывает и ограничена сверху, значит она сходится и потому ряд сходится по определению.

{\bf Признак сравнения с мажорантой}

Пусть даны два положительных ряда $\sum\limits_{k=1}^{\infty} a_{k}$ и $\sum\limits_{k=1}^{\infty} b_{k}$. Если начиная с некоторого номера $n > N$ выполняется неравенство $0 \le a_n \le b_n$, то:

\begin{itemize}
	\item из сходимости рядя $\sum\limits_{k=1}^{\infty} b_{k}$ следует сходимость ряда $\sum\limits_{k=1}^{\infty} a_{k}$
	\item из расходимости ряда $\sum\limits_{k=1}^{\infty} a_{k}$ следует расходимость $\sum\limits_{k=1}^{\infty} b_{k}$
\end{itemize}

{\bf Док-во:}

Из неравенств на члены следует неравенство на частичные суммы $0 \le S_n \le \sigma_n$, дальше очев.


{\bf Признак Раабе}

Если для ряда $\sum\limits_{k=1}^{\infty} a_{k}$ существует предел $$R = \lim\limits_{n \rightarrow \infty} n (\frac{a_n}{a_{n+1}} - 1)$$, то при $R > 1$ ряд сходится, а при $R < 1$ -- расходится. Если $R = 1$, то жанный признак не говорит ничего.

{\bf Признак Гаусса}

Пусть для знакоположительного ряда $\sum\limits_{n=1}^{\infty} a_{n}$ отношение $\frac{a_n}{a_{n + 1}}$ может быть представлено в виде $$\frac{a_n}{a_{n + 1}} = \lambda + \frac{\mu}{n} + \frac{\theta_n}{n^2}$$, где $\lambda, \mu$ -- постоянные, а последовательность $\theta_n$ ограничена. Тогда 
\begin{itemize}
	\item ряд расходится если либо $\lambda > 1$, либо $\lambda = 1, \mu > 1$
	\item ряд расходится, если либо $\lambda < 1$, либо $\lambda = 1, \mu \le 1$
\end{itemize}


{\bf Знакопеременные ряды}

\D{Знакопеременными называются ряды, члены которых могут (стоять) быть как положительными, так и отрицательными.}


{\bf Признак Даламбера}

Слабее признака Коши, но зато проще

Если существует $\lim\limits_{n \rightarrow \infty}|\frac{a_{n + 1}}{a_n}| = r$, то 

\begin{itemize}
	\item если $r < 1$, то ряд абсолютно сходится
	\item если $r > 1$, то ряд расходится
	\item если $r = 1$, то данный признак ничего не говорит (сука)
\end{itemize}

{\bf Док-во:}

1. Пусть начиная с некоторого номера N верно неравенство $|\frac{a_{n+1}}{a_n}| \le q, 0 < q < 1$. Тогда перемножив члены начиная с N будем иметь что $\frac{a_{N+n}}{a_N} \le q^n$ откуда $|a_{N+n}| \le |a_{N}q^n|$, значит ряд $|a_{N+1}| + |a_{N+2}| + ...$ меньше бесконечной суммы убывающей геометрической прогрессии, поэтому он сходится

2. $|\frac{a_{n + 1}}{a_n}| \ge 1$ (с некоторого N), тогда можно записать $|a_{n+1}| \ge |a_n|$ значит модуль членов $a$ не стремится к 0 на бесконечности, значит последовательность не стремится к 0 а значит ряд не сходится.

3. Если просто меньше 1 до там хуйня какая-то мне впадлу
\\

{\bf Радикальный признак Коши} (ебаная оппозиция)

Если существует $\lim\lim\limits_{n \rightarrow \infty} \sqrt[n]{|a_n|} = r$, то

\begin{itemize}
	\item если $r < 1$ то ряд сходится абсолютно
	\item если $r > 1$ то ряд расходится
	\item если $r = 1$ то хз (опять??)
\end{itemize}

{\bf Док-во:} \href{https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B4%D0%B8%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA_%D0%9A%D0%BE%D1%88%D0%B8}{тут}
\\

{\bf Признак Лейбница}

Пусть для знакочередующегося ряда $$S = \sum\limits_{n=1}^{\infty}(-1)^{n-1}a_n, a_n \ge 0$$
выполняются следующие условия

\begin{itemize}
	\item С некоторого $N$ последовательность $a$ монотонно убывает, т.е. $a_{n+1} \le a_n$
	\item $\lim\limits_{n \rightarrow \infty}a_n = 0$
\end{itemize}

Тогда такой ряд сходится

{\bf Док-во:} \href{https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BD%D0%B8%D1%86%D0%B0_%D0%BE_%D1%81%D1%85%D0%BE%D0%B4%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D0%B8_%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D1%87%D0%B5%D1%80%D0%B5%D0%B4%D1%83%D1%8E%D1%89%D0%B8%D1%85%D1%81%D1%8F_%D1%80%D1%8F%D0%B4%D0%BE%D0%B2}{здесь}\\

{\bf Признак Абеля}

\T {Числовой ряд $\sum\limits_{n=1}^{\infty}a_nb_n$ сходится, если выполнены следующие условия
	
	\begin{itemize}
		\item Последовательность \{$a_n$\} монотонна и ограничена
		\item Ряд $\sum\limits_{n=1}^{\infty}b_n$ сходится
	\end{itemize}
}
{\bf Proof:} \href{https://ib.mazurok.com/2015/06/16/%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B8-%D0%B0%D0%B1%D0%B5%D0%BB%D1%8F-%D0%B8-%D0%B4%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5/}{вот}\\

{\bf Признак Дирихле}

\T{Пусть выполнены условия:
	\begin{itemize}
		\item последовательность частичных сумм $B_n = \sum\limits_{k=1}^{n}$ ограничена
		\item последовательность $a_n$, начиная с некоторого номера, монотонно убывает $a_n \ge a_{n+1}$
		\item $\lim\limits_{n\rightarrow\infty}a_n = 0$
	\end{itemize}
	Тогда ряд $\sum\limits_{n=1}^{\infty}a_nb_b$ сходится
}

{\bf Proof:} \href{https://ib.mazurok.com/2015/06/16/%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B8-%D0%B0%D0%B1%D0%B5%D0%BB%D1%8F-%D0%B8-%D0%B4%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5/}{вот}\\

\subsection{Кратные, поверхностные и криволинейные интегралы. Формулы Грина, Стокса и Остроградского}


\subsubsection{Интеграль4ики}
\D{Пусть дана $f(x)$ -- функция действительной переменной. {\bf Неопределенным интегралом} функции $f(x)$, или ее первообразной, называется такая функция $F(x)$, производная которой равна $f(x)$, т.е. $F^{'}(x) = f(x)$. Обозначается $F(x) = \int f(x)dx$ }


\D{Кратным интегралом называют множество интегралов, взятых от $d > 1$, например  $$\underbrace{\int...\int f(x_1,...,x_d)dx_{1}...dx_{d}}_{d}$$}

Замечание -- кратный интеграл -- определенный интеграл, при его вычислении всегда получается число

\D{Криволинейный интеграл -- интеграл вычисляемый вдоль какой-либо прямой.
	
	Пусть $l$ -- пгладкая, без особых точек и пересечений кривая (может быть замкнутой), заданая параметрически $l: r(t)$, где $r$ -- радиус вектор, конец которого описывает кривую, а параметр $t$ направлен от начального значения $a$ к конечному значению $b$. Для интеграла второго рода направление, в котором движется параметр, определяет направление кривой $l$.
	
	Также есть скалярная или векторная функция , которая рассматривается вдоль кривой $l: f(r)$
	
	Еще есть разбиение отрезка параметризации, и разбиение кривой. Они соответствуют друг-другу (параметризация от параметра по факту сопостовляет точке из отрезка параметризации $[a, b]$ точку на прямой, и по разбиению параметризации разбивается кривая по соответствующим точкам, подробнее можно почитать на вики ссылку вставить не получилось:( )
	
	Интегральная сумма для интеграла {\bf первого рода} -- сумма вида $\sum\limits_{k=1}^{n} f(r(\xi_i))\cdot|l_k|$ где $|l_k|$ -- длина соответствующего отрезка, $\xi_i$ -- точка на соответствующем отрезке
	
	Интегральная сумма для интеграла {\bf первого рода} -- сумма вида $\sum\limits_{k=1}^{n} f(r(\xi_i))\cdot(r(t_k) = r(t_{k - 1})))$
	
	Собственно, криволинейный интеграл это интегральная сумма с $n$ устремленным в бесконечность
}

Похоже на обычный определенный интеграл, только тут мы вместо оси выравниваемся на кривую какую-то, и по факту считаем площидь криволинейного цилиндра между кривой в пространстве и ее проекцией (вроде бы, но это не точно)


\D{Пусть $\Phi$ -- гладкая, ограниченная полная поверхность. Пусть далее на $\Phi$ задана функция $f(M) = f(x, y, z)$. Рассмотрим разбиение $T$ этой поверхности на часть $\Phi_i (i=1, ..., n)$ кусочно-гладкими кривыми и на каждой такой части выберем произвольную точку $M_i(x_i, y_i, z_i)$. Вычислив значение функции в этой точке $f(M_i) = f(x_i, y_i, z_i)$ и, приняв за $\sigma_i$ площадь поверхность $\Phi_i$, рассмотрим сумму $$I\{\Phi_i, M_i\} = \sum_i f(M_i)\sigma_i$$. Тогда число I называется пределом сумм $i\{\Phi_i, M_i\}$ если $$\forall \epsilon > 0 \exists \delta > 0 \forall T: d(T) < \delta \forall \{M_i\} |I\{\Phi_i, M_i\} - I| < \epsilon$$
	Предел $I$ сумм $I\{\Phi_i, M_i\}$ при $d(T) \rightarrow 0$
	называется {\bf поверхностным интегралом первого рода} от функции $f(M)$ по поверхности $\Phi$ и обозначается $$I = \iint\limits_{\Phi} f(M)d\sigma$$} 

По сути -- берем поверхность в пространстве, а дальше как в криволинейном -- вместо отрезков оже куски пространства и т.д. получается магия какая-то.

\subsubsection{Формула Грина}

\D{Пусть $C$ -- положительно ориентированная кусочно-гладкая замкнутая кривая на плоскости, а $D$ -- область, ограниченная кривой $C$. Если фунеции $P = P(x, y)$, $Q = Q(x, y)$ определены в области $D$ и имеют неприрывные частные производные $\frac{\partial P}{\partial y}, \frac{\partial Q}{\partial x}$, то $$\oint Pdx + Qdy = \iint\limits_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy$$}
{\bf Док-во и еще:} \href{https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%93%D1%80%D0%B8%D0%BD%D0%B0}{here}

\subsubsection{Формула Стокса}

\D{Пусть на ориентируемом многообразии $M$ размерности $n$ заданы положительно ориентированное ограниченное $p-$мерное подмногообразие $\sigma (1 \le p \le n)$ и дифференциальная форма $\omega$ степени $p - 1$ класса $C^1$. Тогда если граница подмногообразия $\partial \sigma$ положительно ориентированаб то $$\int\limits_{\sigma}d\omega = \int\limits_{\partial \sigma \omega}$$}

Грубо говоря взяли поверхность, и с помощью дифференциалов перешли к интегралу по границе поверхности, как-то так, но надо глубже разбираться потому что очень много определений которые надо помнить

\subsubsection{Формула Остроградского (Гаусс сосать)}

\D{Пусть теперь $\partial V$ -- кусочно-гладкая гипперповерхность $(p = n - 1)$, ограничивающая некоторую область $V$ в $n-$мерном пространстве. Тогда интеграл дивергенции (это оператор который отображает векторное поле на скалярное -- $div F = \lim\limits_{V \rightarrow 0} \frac{\Phi_F}{V}$, где $\Phi_F$ -- поток векторного поля $F$ через сферическую поверхность площадью $S$ ограничивающую объем $V$, хуита какая-то хочу объяснение на пальцах) поля по области равен потоку поля через границу области $\partial V$: $$\int\limits_{V} div F dV = \int\limits_{\partial V} F d \Sigma$$.
	
	В трехмерном пространстве $(n = 3)$ с координатами $\{x, y, z\}$ эквивалентнно $$\int\limits_{\partial V} F d \Sigma = \int\limits_{V}(\frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z}) dV$$, или $$\iiint\limits_{\partial V} Pdydz + Qdzdx + Rdxdy = \iint\limits_{V} (\frac{\partial P}{\partial x} + \frac{\partial Q}{\partial y} + \frac{\partial R}{\partial z})dxdydz$$ }


Понятно что тут взяли и применили стокса на какой-то случай, но чет пиздец ребята)))


\subsection{Функциональные ряды, свойства равномерно сходящихся функциональных рядов. Степенные ряды. Ряд Тейлора.}

\subsubsection{Функциональные ряды}

\D {Функциональный ряд -- ряд, каждым членом которого является функция $u_k(x)$
	
	Обозначается $\sum\limits_{k=1}^{\infty} u_k(x)$}

Функциональная последовательность $u_k(x)$ сходится {\bf поточечно} к функции $u(x)$, если $\forall x \in E \exists \lim\limits_{k \rightarrow \infty} u_k(x) = u(x)$

{\bf Равномерная сходимость} -- существует функция $u(x): E \rightarrow \mathbb{C}$ такая, что 

$sup |u_k(x) - u(x)| \xrightarrow {k \rightarrow \infty} 0, x \in E$

Функциональный ряд называется сходящимся {\bf поточечно}, если последовательность $S_n(x) = \sum\limits_{k=1}^{n} u_k(n)$ сходится поточечно. Аналогично для равномерной сходимости.

{\bf Необходимое условие равноменой сходимости ряда}

$u_k(x) \rightrightarrows 0$ при $k \rightarrow \infty$

Или, что эквивалентно $\forall \epsilon > 0 \exists n_0(\epsilon) \in \mathbb{N} : \forall x \in X, \forall n > n_0 |u_n(x)| < \epsilon$, где $X$ -- область сходимости

{\bf Свойства}

\begin{enumerate}
	\item {\bf Теоремы о непрерывности}
	
	Последовательность непрерывных в точке функций сходится к функции, непрерывной в этой точке.
	
	Последовательность $u_k(x) \rightrightarrows u(x)$
	
	$\forall k:$ функция $u_k(x)$ непрерывна в точке $x_0$
	
	Тогда и $u(x)$ непрерывна в $x_0$
	
	Ряд непрерывных в точке функций сходится к функции, непрерывной в этой точке.
	
	Ряд $\sum\limits_{k=0}^{\infty}u_k(x) \rightrightarrows S(x)$
	
	$\forall k$: функция непрерывна в точкке $x_0$
	
	Тогда $S(x)$ непрерывна в  $x_0$
	
	\item {\bf Теоремы об интегрировании}
	
	Рассматриваются действительнозначные функции на отрезке действительной оси
	
	{\it Теорема о переходе к пределу под знаком интеграла}
	
	$\forall k:$ функция $u_k(x)$ непрерывна на отрезке $[a, b]$
	
	$u_k(x) \rightrightarrows u(x)$ на $[a, b]$
	
	Тогда числовая последовательность $\{\int\limits_{a}^{b} u_k(x) dx\}$ сходится к конечному пределу $\int\limits_a^b u(x) dx$
	
	{\it Теорема о почленном интегрировании}
	
	$\forall k:$ функция $u_k(x)$ непрерывна на отрезке $[a, b]$
	
	$\sum\limits_{k=1}^{\infty}u_k(x) \rightrightarrows S(x)$ на $[a, b]$
	
	Тогда числовой ряд $\sum\limits_{k=1}^{\infty}\int\limits_{a}^{b} u_k(x) dx$ сходится и равен $\int\limits_a^b S(x) dx$
	
	\item {\bf Теоремы о дифференцировании}
	
	Рассматриваются действительнозначные функции на отрезке действительной оси
	
	{\it Теорема о дифференцировании под пределом}
	
	$\forall k:$ функция $u_k(x)$ дифференцируема (имеет непрерывную производную) на отрезке $[a, b]$
	
	$\exists c \in [a, b]: u_k(c)$ сходится к конечному пределу
	
	$u_k^{\prime}(x)  \rightrightarrows \omega(x)$ на отрезке $[a, b]$
	
	Тогда $\exists u(x): u_k(x) \rightrightarrows u(x),\ u(x)$ -- дифференцируема на $[a, b],\ u^{\prime}(x) = \omega(x)$ на $[a, b]$
	
	{\it Теорема о почленном дифференцировании}
	
	$\forall k:$ функция $u_k(x)$ -- дифференцируема на отрезке $[a, b]$
	
	$\exists c \in [a, b]: \sum\limits_{k=1}^{\infty} u_k(c)$ сходится
	
	$\sum\limits_{k=1}^{\infty}u_k^{\prime}(x)$ равномерно сходится на отрезке $[a, b]$
	
	Тогда $\exists S(x): \sum\limits_{k=1}^{\infty}u_k(x) \rightrightarrows S(x),\ S(x)$ -- дифференцируем на $[a, b], S^{\prime}(x) = \sum\limits_{k=1}^{\infty}u_k^{\prime}(x)$ на $[a, b]$
	
\end{enumerate}

\subsubsection{Степенные ряды}

\D {{\bf Степенной ряд с одной переменной} -- это формальное алгебраическое вырадение вида $$F(x) = \sum\limits_{n=0}^{\infty}a_nX^n$$ в котором коэффициенты $a_n$ берутся из некоторого кольца $R$, обычно вещественные или комплексные числа}

Для степенных рядов есть несколько теорем об их сходимости

\begin{itemize}
	\item Певая теорема Абеля
	
	Пусть ряд $\sum a_n x^n$ сходится в точке $x_0$. Тогда этот ряд сходится абсолютно в круге $|x| < |x_0|$ и равномерно по $x$ на любом компактном подмножестве этого круга.
	
	Отсюда можно сделать вывод что если ряд расходится при $x = x_0$, то он расходится при всех $|x| > |x_0|$
	
	Появляется понятие радиуса сходимости $R$, при котором при $|x| < R$ ряд сходится абсолютно, про $|x| > R$ расходится
	
	\item Формула Коши-Адамара (Коши-Амидамару)
	
	Значение радиуса сходимости степенного ряда может быть вычислено по формуле $\frac{1}{R} = \uplim\limits_{n \rightarrow +\infty}|a_n|^{1/n}$
	
	\item Признак Даламбера
	
	Если при $n > N$ и $\alpha > 1$ выполнено неравенство $|\frac{a_n}{a_{n+1}}| \ge R(1 + \frac{\alpha}{n})$ тогда степенной ряд $\sum a_n x^n$ сходится во всех точках окружности $|x| = R$ абсолютно и равномерно по $x$
	
	\item Признак Дирихле
	
	Если все коэффициенты степенного ряда $\sum a_n x^n$ положительны и последовательность $a_n$ монотонно сходится к 0б тогда этот ряд сходится во всех точках окружности $|x| = 1$, кроме, может быть, точки $x = 1$
\end{itemize}


\subsubsection{Ряд Тейлора}

\D {Ряд Тейлора -- разложение функции в бесконечную сумму степенных функций
	
	Многочленом Тейлора функции $f(x)$ вещественной переменной $x$, дифференцируемой $k$ раз в точке $a$ называется конечная сумма 
	$$f(x) = \sum\limits_{n=0}{k}\frac{f^{(n)}(a)}{n!} (x - a)^n = f(a) + f^{\prime}(a)(x - a) + \frac{f^{(2)}(a)}{2!}(x - a)^2 + ... + \frac{f^{(k)}(a)}{k!}(x - k)^k$$
	
	Рядом Тейлора в точке $a$ функции $f(x)$ , бесконечно диффиренцируемой в окрестности точки $a$, называется формальный степенной ряд
	$$f(x) = \sum\limits_{n=0}^{+\infty}\frac{f^{(n)}(a)}{n!}(x - a)^n$$
	
	Другими словами, рядом Тейлора функции $f(x)$ в точке $a$ называется ряд разложения функции по положительным степеням двучлена $(x - a)$
	
}

Еще есть формула Тейлора, это просто частичная сумма ряда вроде как.

В случае $a = 0$ это все безобразие -- {\bf ряд Маклорена}


\subsection{Определители и их свойства. Системы линейных алгебраических уравнений и их исследование. Методы решения систем линейных алгебраических уравнений.}

\subsubsection{Определитель}

\D {Определитель -- скалярная величина, которая характеризует ориентированное "растяжение"\ или "сжатие" \  многомерного евклидова пространства после преобразования матрицей. Имеет смысл только для квадратных матриц. Стандартные обозначения -- $\det(A), |A|, \Delta (A)$ }

{\bf Определение через перестановки}

Для квадратной матрицы $A = (a_{ij})$ размера $n \times n$ ее определитель вычисляется по формуле $$det A = \sum\limits_{\alpha_1, \alpha_2, ..., \alpha_n}(-1)^{N(\alpha_1, \alpha_2, ..., \alpha_n)} \cdot a_{1 \alpha_1} a_{2 \alpha_2} ... a_{n \alpha_n}$$

Где суммирование проводится по всем перестановкам $\alpha_1, \alpha_2, ..., \alpha_n$ чисел $1, 2, ..., n$, а $N(\alpha_1, \alpha_2, ..., \alpha_n)$ обозначает число инверсий в перестановке $\alpha_1, ..., \alpha_n$

Таким образом в определитель входит $n!$ слагаемых.


{\bf Аксиоматическое построение}

Понятие определителя может быть введено на основе его свойств. А именно, определителем вещественной матрицы называется функция $det: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$, обладающая следующими тремя свойствами

\begin{itemize}
	\item $\det(A)$ -- кососимметрическая функция строк(столбцов) матрицы $A$, т.е. не меняется при четных перестановках аргументов
	\item $\det(A)$ -- полилинейная функция строк (столбцов) матрицы $A$
	\item $\det(E) = 1$, где $E$ -- единичная $n \times n$ матрица.
\end{itemize}

Еще свойства

\begin{enumerate}
	\item $\det E = 1$
	\item $\det cA = c^n \det A$
	\item $\det A^T = \det A$
	\item $\det(AB) = \det A \cdot \det B$
	\item $\det A^{-1} = (\det A)^{-1}$, причем матрица обратима тогда и только тогда, когда обратим ее определитель
	\item Существует ненулевое решение уравнения $AX = 0$ тогда и только тогда, когда $\det A = 0$ 
\end{enumerate}

\subsubsection{Системы линейных уравнений}

В классическом варианте коэффициенты при переменных, свободные члены и неизвестные считаются вещественными числами

Общий вид системы линейных алгебраических уравнений:

$$
{\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\dots +a_{1n}x_{n}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\dots +a_{2n}x_{n}=b_{2}\\\dots \\a_{m1}x_{1}+a_{m2}x_{2}+\dots +a_{mn}x_{n}=b_{m}\\\end{cases}}$$

где $m$ -- количество уравнений, а $n$ -- количество переменных.

Система называется {\bf однородной}, если все ее свободные члены ($b_i$) равны нулю, иначе -- {\bf неоднородной}

Система называется {\bf совместной}, если она имеет хотя бы одно решение, иначе несовместной. Решения считаются различными, если хотя бы одно из значений переменных не совпадает. Если решение одно, то система {\bf определенная}

Также есть запись в матричной форме

$
\begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{pmatrix}
\begin{pmatrix}
	x_1 \\
	x_2 \\
	\vdots \\
	x_n
\end{pmatrix} 
=
\begin{pmatrix}
	b_1 \\
	b_2 \\
	\vdots \\
	b_m
\end{pmatrix}$

или $Ax=b$. Если к матрицу $A$ приписать справа столбец свободных членов, матрица будет называться расширенной.

Системы называются {\bf эквивалентными}, если множество их решений совпадает, т.е. если решение одной системы является решением другой.

Можно менять уравнения домножением на константу кроме 0, на сумму с другим уравнением, на линейную комбинацию с учетом этой. Будут получаться эквивалентные системы.

\subsubsection{Методы решения систем уравнений}

\begin{itemize}
	\item {\it Метод Гаусса}
	
	Приводим матрицу к ступенчатому виду, остались какие-то переменные. Назовем главными те, которые на диагонали, остальные -- свободные. Теперь переносим свободные через =, и присваивая им все возможные значения легко получить решения для главных, а значит для всей системы.
	
	Подробнее \href{https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%B0}{здесь}
	
	\item {\it Метод Гаусса-Жордана}
	
	\begin{enumerate}
		\item Выбирают первый слева столбец матрицы, в котором есть хоть одно отличное от нуля значение.
		\item Если самое верхнее число в этом столбце ноль, то меняют всю первую строку матрицы с другой строкой матрицы, где в этой колонке нет нуля.
		\item Все элементы первой строки делят на верхний элемент выбранного столбца.
		\item Из оставшихся строк вычитают первую строку, умноженную на первый элемент соответствующей строки, с целью получить первым элементом каждой строки (кроме первой) ноль.
		\item Далее проводят такую же процедуру с матрицей, получающейся из исходной матрицы после вычёркивания первой строки и первого столбца.
		\item После повторения этой процедуры $(n-1)$ раз получают верхнюю треугольную матрицу
		\item Вычитают из предпоследней строки последнюю строку, умноженную на соответствующий коэффициент, с тем, чтобы в предпоследней строке осталась только 1 на главной диагонали.
		\item Повторяют предыдущий шаг для последующих строк. В итоге получают единичную матрицу и решение на месте свободного вектора (с ним необходимо проводить все те же преобразования).
		
	\end{enumerate}
	
	Короче приводим к единичной, что осталось у свободных членов и есть решение
	
	\item {\it Метод Крамера}
	
	Для системы $n$ линейных уравнений с $n$ неизвестными (над произвольным полем)
	
	$\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\ldots +a_{1n}x_{n}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\ldots +a_{2n}x_{n}=b_{2}\\\cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \\a_{n1}x_{1}+a_{n2}x_{2}+\ldots +a_{nn}x_{n}=b_{n}\\
	\end{cases}$
	
	с определителем матрицы системы $\Delta$ , отличным от нуля, решение записывается в виде
	
	$ x_{i}={\frac {1}{\Delta }}{\begin{vmatrix}a_{11}&\ldots &a_{1,i-1}&b_{1}&a_{1,i+1}&\ldots &a_{1n}\\a_{21}&\ldots &a_{2,i-1}&b_{2}&a_{2,i+1}&\ldots &a_{2n}\\\ldots &\ldots &\ldots &\ldots &\ldots &\ldots &\ldots \\a_{n-1,1}&\ldots &a_{n-1,i-1}&b_{n-1}&a_{n-1,i+1}&\ldots &a_{n-1,n}\\a_{n1}&\ldots &a_{n,i-1}&b_{n}&a_{n,i+1}&\ldots &a_{nn}\\\end{vmatrix}}$
	(i-ый столбец матрицы системы заменяется столбцом свободных членов).
	
	Подставляем вместо соответствующего столбца свободные члены, считаем определитель, делим на определитель всей матрицы, и получаем $x_i$ соответствующий данному столбцу.
	
	\item {\it Матричный метод}
	
	Есть система вида $AX=B$, тогда решением будет $X=A^{-1}B$
	
	Чтобы работало, нужно чтобы матрица $A$ была невырождена, т.е. чтобы определитель был не равен 0
	
\end{itemize}

Еще есть какие-то итерационные и другие методы, но они звучат и выглядят не очень полезными, но можно посмотреть \href{https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0_%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D1%85_%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9#:~:text=%D0%A1%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0%20%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D1%85%20%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85%20%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9%20(%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F,%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%BC%20%E2%80%94%20%D0%B0%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%BC%20%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC%20%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B9%20%D1%81%D1%82%D0%B5%D0%BF%D0%B5%D0%BD%D0%B8}{тут}

\subsection{Линейные операторы в конечномерном пространстве и их матричноепредставление. Характеристический многочлен, собственные числа и собственные вектора линейного оператора. Сопряженные и самосопряженные операторы.}

\subsubsection{ Линейные операторы}

\D {Пусть $X$ и $Y$ -- линейные пространства над полем $F$. Отображение $\mathcal{A}: X \rightarrow Y$ называется линейным оператором, если $\forall x_1, x_2 \in X, \forall \lambda \in F$
	\begin{itemize}
		\item $\mathcal{A}(x_1 + x_2) = \mathcal{A}(x_1) + \mathcal{A}(x_2)$
		\item $\mathcal{A}(\lambda \cdot x_1) = \lambda \cdot \mathcal{A}(x_1)$
	\end{itemize}
	
	Линейный оператор $\mathcal{A}: X \rightarrow X$ называется автоморфизмом (или гомоморфизмом)
	
}

Операторы равны, если переводят элементы первого пространства в одинаковые элементы второго пространства.

\D{Пусть $\mathcal{A}: X \rightarrow Y$
	
	Пусть п.п. $X \leftrightarrow \{e_k\}_{k=1}^{n},\ \dim X = n$
	
	Пусть п.п. $Y \leftrightarrow \{h_k\}_{k=1}^{n},\ \dim Y = m$
	
	$\underset{{1\le k \le n}}{\mathcal{A}e_k} = \sum\limits_{i = 1}^{m} \alpha_k^i \cdot h_i \Rightarrow A = ||\alpha_k^i||,$ где $1 \le i \le m, 1 \le k \le m$
	
	$A = \begin{pmatrix}
		\alpha_1^1 & ... & \alpha_n^1\\
		\alpha_1^2 & ... & \alpha_n^2\\
		... & ... & ...\\
		\alpha_1^n & ... & \alpha_n^n\\
		
	\end{pmatrix}$
	
}

\subsubsection{Характеристический многочлен}

\D {Для данной матрицы $A$, $\chi(\lambda) = \det (A - \lambda E)$, где $E$ -- единичная матрица, является многочленом от $\lambda$, который называется {\bf характеристическим многочленом} матрицы $A$ (видимо можно отождествить матрицу с линейным оператором, тогда будет многочлен для оператора)}

Ценность характеристического многочлена в том, то собственные значения матрицы являются его корнями. Действительно, если уравнение $Av = \lambda v$ имеет ненулевое решение, то $(A - \lambda E)v = 0$, значит матрица $A - \lambda E$ вырождена и ее определитель $\det (A - \lambda E) = \chi(\lambda)$ равен 0

{\bf Свойства}

\begin{itemize}
	\item Для матрицы $n \times n$ характеристический многочлен имеет степень $n$
	\item Все корни характеристического многочлена матрицы являются ее собственными значениями
	\item Теорема Гамильтона-Кэли -- если $\chi(\lambda)$ -- характеристический многочлен матрицы $A$, то $\chi(A) = 0$
	\item Характеристические многочлены подобных матриц совпадают
	\item Характеристический многочлен обратной матрицы $\chi_{A^{-1}}(\lambda) = \frac{(-\lambda)^n}{\det A}\chi_A(1/\lambda)$
	\item Если $A$ и $B$ две матрицы $n \times n$, то $\chi_{AB} = \chi_{BA}$. В частности $tr(AB) = tr(BA), \det(AB) = \det (BA)$
	\item В более общем виде, если $A$ --матрица $m \times n$, а $B$ -- матрица $n \times m$, причем $m < n$, так что $AB$ и $BA$ --квадратные матрицы размеров $m$ и $n$ соответственно, то $\chi_{BA}(\lambda) = \lambda ^{n - m}\chi_{AB}(\lambda)$ 
\end{itemize}


\D{Пусть $L$ -- линейное пространство над полем $K$,
	
	$\mathcal{A}:L \rightarrow L$ -- линейный оператор
	
	{\bf Собственным вектором} линейного оператора $\mathcal{A}$ называется такой ненулевой вектор $x \in L$, что для некоторого $\lambda \in K: \mathcal{A}x = \lambda x$
	
	При этом $\lambda$ называют {\bf собственным числом} оператора $\mathcal{A}$
	
}

{\bf Свойства}

\begin{itemize}
	\item Собственные векторы, отвечающие различным собственным значениям, образуют ЛНЗ набор
	\item Еще какие-то леммы есть, подробнее см на \href{https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%8B_%D0%B8_%D1%81%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B5_%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F}{говне}
\end{itemize}

\subsubsection{Сопряженные и самосопряженные операторы}

\D{Пусть $E, L$ -- линейные пространства, а $E^*, L^*$ -- сопряженные линейные пространства (пространства линейных функционалов, определенных на $E$ и $L$). Тогда для любого линейного оператора $\mathcal{A}: E \rightarrow L$ и любого линейного функционала $g \in L^*$ определен линейный функционал $F \in E^*$ -- суперпозиция $g$ и $A: f(x) = g(A(x))$. Отображение $g \rightarrow f$ называется сопряженным линейным оператором и обозначается $\mathcal{A^*}: L^* \rightarrow E^*$. Если кратко, то $(\mathcal{A^*}g, x) = (g, \mathcal{A}x)$
	
	Если же $\mathcal{A^*} = \mathcal{A}$, то такой оператор называется самосопряженным, для него $(\mathcal{A}x, y) = (x, \mathcal{A}y)$
	
}

\subsection{Задача Коши для системы обыкновенных дифференциальных уравнений. Существование и единственность решения. Устойчивость.}

\subsection{Линейные обыкновенные дифференциальные уравнения и системы. Фундаментальная система решений. Метод вариации постоянных для решения неоднородных уравнений.}

\subsection{Дискретные случайные величины. Математическое ожидание и дисперсия. Стандартные дискретные распределения (Бернулли, биномиальное, геометрическое, Пуассона).}

\D { Случайная величина -- отображение из множества элементарных исходов в множество вещественных чисел}

\D { {\bf Дискретной случайной величиной} называется случайная величина, множество значений которой не более чем счетно, причем принятие ею каждого из значений есть случайное событие с определенной вероятностью}

\D {{\bf Функция распределения} случайной величины -- функция $F(x)$, определенная на $\mathbb{R}$ как $P(\xi \le x)$ т.е. выражаюшая вероятность того, что $\xi$ примет значение меньшее или равное $x$}

Матожидание дискретной величины -- $E\xi = \sum \xi_i p(\xi_i)$, где $\xi_i$ -- возможный исход, а $p_{\xi_i}$ -- его вероятность.

Дисперсия пересчитывается по формуле $DX = EX^2 - (EX)^2$

\subsubsection{ Стандартные дискретные распределения}

{\bf Распределение Бернулли}

Величина принимает всего 2 значения -- $1$ и $0$ с вероятностями $p$ и $q = 1 - p$ соответственно

{\bf Биноминальное распределение}

Это распределение количества "успехов" \ в последовательности из $n$ независимых случайных экспериментов, таких, что вероятность успеха в них одинакова и равна $p$.

Т.е. есть последовательность $X_1, ..., X_n$ -- независимых случайных величин имеющих распределение Бернулли с параметром $p$, тогда $Y = X_1 + ... + X_n$ имеет биноминальное распределени с параметрами $n$ и $p = Bin(n, p)$ 

В этом случае функция вероятности задается формулой $\mathbb{P}(Y = k) = {n \choose k} p^k q^{n - k}, k = 0, ..., n$

${n \choose k} = C_n^k = \frac{n!}{k!(n-k)!}$

{\bf Геометрическое распределение}

Одно из двух

\begin{enumerate}
	\item распределение вероятностей случайной величины $X$ равой номеру первого успеха в серии испытаний Бернулли и принимающей значение $n = 1, 2, 3,...$
	\item Распределение вероятностей случайной величины $Y = X - 1$ равной числу неудач до первого успеха, и принимающей значения $n = 0, 1, 2,...$
\end{enumerate}

В cлучае первого успеха $P(X = n) = (1 - p)^{n - 1}p$

В случае количества неудач -- $P(Y = n) = (1 - p)^n p$


{\bf Распределение Пуассона}

Распределение числа событий, произошедших за фиксированое время, при условии что они происходят с некоторой фиксированной средней интенсивностью и независимо друг от друга

Выберем фиксированное $\lambda > 0$ и определим дискретное распределение по функции вероятности:

$p(k) = P(Y = k) = \frac{\lambda^k}{k!}e^{-\lambda}$

где 

$k$ -- количество событий

$\lambda$ -- матожидание величины 



\subsection{Непрерывные случайные величины и их функции распределения. Математическое ожидание и дисперсия. Стандартные непрерывные распределения (равномерное, показательное, нормальное).}

\subsubsection{Непрерывные случайные величины}

\D { Непрерывной случайной величиной называют случайную величину, которая в результате испытания принимает все значения из некоторого числового промежутка. Число возможных значений непрерывной случайной величины бесконечно.}

$F_X(x) = P(X \le x)$


В непрерывном случае $\P(X = x) = 0 \forall x \in \mathbb{R}$
и $F_X(x - 0) = F_X(x), \forall x \in \mathbb{R}$

А следовательно формулы имеют вид $P(X \in |a, b|) = F_X(b) - F_X(a)$



Матожидание для непрерывных величин считается по формуле $EX = \int\limits_{-\infty}^{+\infty}xf(x)dx$, где $f(x)$ -- функция плотности распределения (штука от которой интеграл равен 1, говорит с какой вероятностью выпадет то или иное значение (или отношение для разных значений в непрерывном случае))


Дисперсия же вычисляется по формуле $DX = \int\limits_{-\infty}^{+\infty} x^2 f(x) dx - (EX)^2$

\subsubsection{Стандартные равномерные распределения}

{\bf Равномерное распределение}

Распределение, которое принимает значения из некоторого промежутка конечной длины, при этом плотность вероятности на этом промежутке почти всюду постоянна.

Т.е. с равной вероятностью может выпасть любое значение из промежутка.

Плотность имеет вид $f_X(x) = \begin{cases}
	\frac{1}{b - a}, x \in [a, b]\\
	0, x \not\in [a, b]
\end{cases}$

{\bf Показательное (экспоненциальное) распределение}

Случайная величина имеет экспоненциальное распределение с параметром $\lambda > 0$, если ее плотность вероятности имеет вид

$f_X(x) = \begin{cases}
	\lambda e^{-\lambda x}, x \ge 0\\
	0, x < 0
\end{cases}$

{\bf Нормальное распределение}


Плотность вероятности $f(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}$

где $\mu$ -- матожидание

$\sigma$ -- среднеквадратическоее отклонение

Данное распределение моделирует ситуацию когда есть какое-то среднее и какая-то дисперсия, Наибольшая вероятность получить среднее, в зависимости от дисперсии получаем значения дальше или ближе к среднему.



\subsection{Вероятностные неравенства Йенсена, Маркова и Чебышёва. Правило трёх сигм. Закон больших чисел}

\subsubsection{Неравенство Йенсена}

\D {Пусть $(\Omega, \mathcal{F}, \mathbb{P})$ -- вероятностное пространство, и $X: \Omega \rightarrow \mathbb{R}$ -- определенная на нем случайная величина. Пусть также $\phi: \mathbb{R} \rightarrow \mathbb{R}$ -- выпуклая (вниз) борелевская функция. Тогда если $X, \phi(X) \in L^1 (\Omega, \mathcal{F}, \mathbb{P})$, то
	$$\phi(EX) \le E(\phi(X))$$. Также можно добавить условность}

\subsubsection{Неравенство Маркова}

\D {Пусть неотрицательная случайная величина $X: \Omega \rightarrow \mathbb{R}^{+}$ определена на вероятностном пространстве, и ее матожидание конечно. Тогда 
	
	$\mathbb{P}(X \ge a) \le \frac{EX}{a}$}

{\bf Док-во:}

Пусть неотрицательная лучайная величина $X$ имеет плотность распределения $p(x)$, тогда для $a > 0$

$EX = \int\limits_{0}^{\infty} xp(x)dx \ge \int\limits_{a}^{\infty}xp(x)dx \ge \int\limits_{a}^{\infty}ap(x)dx = a \mathbb{P}(X \ge a)$


\subsubsection{Неравенство Чебышева}

\D {Пусть случайная величина $X$ определена на вероятностном пространстве, а ее матожидание $\mu$ и дисперсия $\sigma^2$ конечны. Тогда
	
	$P(|X - \mu| \ge a) \le \frac{\sigma^2}{a^2}, a > 0$
	
	Если $a = k\sigma$, где $\sigma$ -- стандартное отклонение, $k > 0$, то получаем
	
	$P(|X - \mu| >\ge k\sigma) \le \frac{1}{k^2}$
}


\subsubsection{Правило трех сигм}

Если случайная величина распределена нормально, то абсолютная величина ее отклонения от матожидания не превосходит утроенного среднего квадратического отклонения

$P(|X - \mu| \ge 3 \sigma) \le \frac{1}{9}$


\subsubsection{Закон больших чисел}

\D {Рассмотрим последовательность независимых в совокупности случайных величин $X_1, X_2, ...$ интегрируемых по Лебегу, которые имеют одинаковые распределения, следовательно, и одинаковые матожидания. Обозначим через $\overline{X}_n$ среднее арифметическое рассматриваемых случайных величин. Оно сходится к матожиданию}

{\bf Слабый закон}

Сходится по вероятности к матожиданию 

$\overline{X}_n \rightarrow \mu$ при $n \rightarrow 0$

\subsection{Множества и операции над ними. Булевы функции, КНФ, ДНФ. Базисы, теорема Поста.}

\subsection{Комбинаторные объекты. Коды Грея. Формула включения-исключения. Лемма Бернсайда и Теорема Пойа. Числа Стирлинга. Подсчёт деревьев. Метод производящих функций.}

\subsection{Детерминированные и недетерминированные конечные автоматы, их эквивалентность. Минимизация ДКА.}

\subsection{Математическая логика. Понятие доказательства. Правила вывода. Теоремы Гёделя.}

\subsection{Контекстно-свободные грамматики. Эффективные методы разбора: LL(k)-, LR(k)- и LALR-грамматики.}

\subsection{Комбинаторная теория сложности. Временная и емкостная сложность. Сложностные классы P, NP, PS. Сведение, NP-полные задачи.}

\subsection{Марковские цепи, Эргодические цепи, Регулярные цепи. Алгоритм Витерби.}

\subsection{Линейные структуры данных. Амортизационный анализ. Поисковые структуры данных. Запросы на отрезках. Персистентные структуры данных.}

\subsection{Графы. Обход графов. Поиск кратчайших путей. Задача о паросочетании, максимальном потоке и максимальном потоке минимальной стоимости.}

\subsection{Строки. Поиск строки в подстроке. Бор, алгоритм Ахо-Корасика. Суффиксные массивы и деревья.}

\subsection{Постановка задачи линейного программирования. Двойственность задачи ЛП.}

\subsection{Градиентные методы. Метод сопряжения градиентов. Минимизация квадратичных функций. Метод Ньютона.}

\newpage

\section{Программирование и вычислительная техника}

\subsection{Архитектура ЭВМ. Архитектура фон Неймана и гарвардская архитектура. Основные принципы и их альтернативы. Архитектура набора команд (ISA), CISC и RISC архитектуры.}

\subsubsection{Архитектура ЭВМ}
\textbf{Архитектура ЭВМ}~---~
это модель, устанавливающая принципы организации вычислительной системы, состав, 
порядок и взаимодействие основных частей ЭВМ, функциональные возможности, 
удобство эксплуатации, стоимость, надежность.

\subsubsection{Архитектура фон Неймана и гарвардская архитектура. Основные принципы и их альтернативы.}


\href{https://www.currentschoolnews.com/ru/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8-%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F/%D0%A0%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0-%D0%BC%D0%B5%D0%B6%D0%B4%D1%83-%D1%84%D0%BE%D0%BD-%D0%9D%D0%B5%D0%B9%D0%BC%D0%B0%D0%BD%D0%B0-%D0%B8-%D0%93%D0%B0%D1%80%D0%B2%D0%B0%D1%80%D0%B4%D1%81%D0%BA%D0%BE%D0%B9-%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D1%8B/}{Подробнее тут}
\\

\textbf{Особенности архитектуры фон Неймана:}
\begin{enumerate}
	\item Архитектура фон Неймана - это теоретический проект, основанный на концепции компьютера с хранимой программой.
	\item Архитектура фон Неймана имеет только одну шину, которая используется как для извлечения инструкций, так и для передачи данных. Что еще более важно, операции должны быть запланированы, потому что они не могут быть выполнены одновременно.
	\item В архитектуре фон Неймана процессору потребовалось бы два тактовых цикла для выполнения инструкции.
	\item Архитектура фон Неймана обычно используется буквально на всех машинах, от настольных компьютеров, ноутбуков, высокопроизводительных компьютеров до рабочих станций.
\end{enumerate}

\textbf{Особенности Гарвардской aрхитектуры:}
\begin{enumerate}
	\item Гарвардская архитектура - это современная компьютерная архитектура, основанная на компьютерной модели ретранслятора Harvard Mark I.
	\item Гарвардская архитектура имеет отдельное пространство памяти для инструкций и данных, которое физически разделяет сигналы и код хранения и память данных, что, в свою очередь, позволяет получить доступ к каждой из систем памяти одновременно.
	\item В гарвардской архитектуре процессор может выполнить инструкцию за один цикл, если были установлены соответствующие планы конвейерной обработки.
	\item Гарвардская архитектура - это новая концепция, используемая специально в микроконтроллерах и цифровой обработке сигналов (DSP).
	\item Гарвардская архитектура - сложный вид архитектуры, поскольку в ней используются две шины для команд и данных, что делает разработку блока управления сравнительно более дорогой.
\end{enumerate}

\subsubsection{Архитектура набора команд (ISA), CISC и RISC архитектуры.}

\textbf{ISA}~---~архитектура набора команд, которая включает в себя систему и режимы адресации, спецификацию команд процессора, ригистры и типы данных, систему прерываний (для обработки ошибок во время вычисления)

Процессоры можно разделить по сложности набора команд:
\emph{CISC} (Complex Inst. Set Computer) vs \emph{RISC} (Reduced Inst. Set Computer).

\textbf{RISC}~---~Главная идея в поддержании небольшого набора простых и быстрых команд, под которые соптимизирован процессор. Предполагалось, что  сложные вызываются редко. Для этой архитектуры характерен фиксированная длина кодов команд, так как их проще декодировать и она не большая.

\textbf{CISC}~---~Тут у нас много команд для всяких сложных операций которые реализованы непосредственно на плате, что позволяет их ускорить, но это усложняет архитектуру и может мешать эффективной реализации простых команд, поэтому по количество операций в секунду такие процессоры проигрывают RICS. Так как количество команды большое, то характерна переменная длина кода (с Хаффман кодированием).

В современном мире по факту используется компромисс между этими подходами. Процессоры - CISC по спецификации, но внутри скорее RICS и конвертируют сложные в более простые + 0-level cache.

\subsection{Архитектура ЭВМ. Кэш-память. Многоуровневая организация кэш-памяти. Протоколы когерентности кэш-памяти}

\subsubsection{Архитектура ЭВМ}
\textbf{Архитектура ЭВМ}~---~
это модель, устанавливающая принципы организации вычислительной системы, состав, 
порядок и взаимодействие основных частей ЭВМ, функциональные возможности, 
удобство эксплуатации, стоимость, надежность.

\subsubsection{Кэш-память. Многоуровневая организация кэш-памяти. Протоколы когерентности кэш-памяти}
\textbf{Кэширование}~---~это использование дополнительной быстродействующей памяти (кэш-памяти) для хранения копий блоков информации из основной (оперативной) памяти, вероятность обращения к которым в ближайшее время велика.

\textbf{Аспекты кэшей}:
\begin{enumerate}
	\item Кэш-линии~---~вся память выровнена и разбита на непересекающиеся отрезки по 64 байта (последние 6 бит не используются как тег в ассоциативном кэше). 
	\item Кэши делятся на уровни (ближе к процессору $\Rightarrow$ больше скорость, меньше размер). 
		\begin{enumerate}
			\item L1 (32k, делится на данные и команды, Associativity = 4)
			\item L2 (256k, Associativity = 8)
			\item L2 (8Mb, Associativity = 16)
		\end{enumerate}
	\item Ассоциативность. Полная асс. - это когда мы просто пишем в кэш и для поиска нужного адреса нужно бежать по всем линиям. Асс =1 - это когда мы просто мапим по хвосту адреса (тегу) в таблицу. (типа хеш-таблица). Тогда быстро искать, но часто будем промахиваться. Если Acc = k - то мапим в корзины по k и получаем компромисс.
	\item  Эксклюзивность/инклюзивность - данные хранятся только в одном кэш (- эффективность из-за поиска, + размер) или они дублируются в уровнях (в памяти) ниже. (+ скорость, - размер)
	\item Когда у нас много процессоров, то возникает необходимость использовать протоколы когерентности. (MSI, MESI, MESIF, MOESI). Это необходимо, чтобы один процессор мог знать о том, что данные изменились только в кеше одно из его соседей. Для этого вводятся разные состояния владения памятью (invalid, shared, modified + owned/forward, exclusive), чтобы как можно более тоньше развести их и поменьше сбрасывать кэши.
\end{enumerate}

\subsubsection{Многоуровневая организация кэш-памяти (подробнее)}

\emph{ВОДА:}

Современные технологии позволяют разместить КЭШ-память и ЦП на общем кристалле. Такая внутренняя КЭШ-память строится по технологии статического ОЗУ и является наиболее быстродействующей. 

Емкость ее обычно не превышает 64 Кбайт. Попытки увеличения емкости обычно приводят к снижению быстродействия, главным образом, из-за усложнения схем управления и дешифрации адреса. 

Общую емкость КЭШ-памяти ЭВМ увеличивают за счет второй (внешней) КЭШ-памяти, расположенной между внутренней КЭШ- памятью и ОЗУ. Такая система известна под названием двухуровневой, где внутренней КЭШ-памяти отводится роль первого уровня (L1), а внешней — второго уровня (L2). Емкость L2 может быть значительной (до 1 МБ). 

При доступе к памяти ЦП сначала обращается к КЭШ-памяти первого уровня. В случае промаха производится обращение к КЭШ-памяти второго уровня. Если информация отсутствует и в L2, выполняется обращение к ОЗУ и соответствующий блок заносится сначала в L2, а затем и в L1. Благодаря такой процедуре часто запрашиваемая информация может быть быстро восстановлена из КЭШ-памяти второго уровня. Для ускорения обмена информацией между ЦП и L2 между ними часто вводят специальную шину, так называемую шину заднего плана, в отличие от шины переднего плана, связывающую ЦП с основной памятью. 

Количество уровней КЭШ-памяти не ограничивается двумя. В некоторых ЭВМ можно встретить КЭШ-память третьего уровня (L3). Ведутся активные дискуссии о введении также и КЭШ-памяти четвертого уровня (L4). Характер взаимодействия очередного уровня с предшествующим аналогичен описанному для L1 и L2. Таким образом, можно говорить об иерархии КЭШ-памяти. Каждый последующий уровень характеризуется большей емкостью, меньшей стоимостью, но и меньшим быстродействием, хотя оно все же выше, чем у ЗУ основной памяти.

\subsection{С++. Процесс компиляции и линковки. .cpp, .h, .i, .o файлы.}
Понятно и подробно описано: 
\href{https://habr.com/ru/post/478124/}{https://habr.com/ru/post/478124/}

Кратко и структурировано: \href{https://server.179.ru/tasks/cpp/total/105.html}{https://server.179.ru/tasks/cpp/total/105.html}

\textbf{Компиляция}~---~трансляция программы, составленной на исходном языке высокого уровня, в эквивалентную программу на низкоуровневом языке, близком машинному коду (абсолютный код, объектный модуль, иногда на язык ассемблера). Входной информацией для компилятора (исходный код) является описание алгоритма или программа на объектно-ориентированном языке, а на выходе компилятора—эквивалентное описание алгоритма на машинно-ориентированном языке (объектный код).

\subsubsection{Заголовочные файлы (.h)}

%Целью заголовочных файлов является удобное хранение набора объявлений объектов для их последующего использования в других программах. 
В языках программирования Си и C++ заголовочные файлы~---~основной способ подключить к программе типы данных, структуры, прототипы функций, перечисляемые типы и макросы, используемые в другом модуле. По умолчанию используется расширение .h; иногда для заголовочных файлов языка C++ используют расширение .hpp.

Чтобы избежать повторного включения одного и того же кода, используются директивы \#ifndef, \#define, \#endif.

Заголовочный файл в общем случае может содержать любые конструкции языка программирования, но на практике исполняемый код (за исключением inline-функций в C++) в заголовочные файлы не помещают.

\subsection{С++. Жизненный цикл объектов в С++. RAII.}

\subsection{Java. Устройство сборщика мусора в JVM.}

\href{https://medium.com/nuances-of-programming/%D1%81%D0%B1%D0%BE%D1%80%D0%BA%D0%B0-%D0%BC%D1%83%D1%81%D0%BE%D1%80%D0%B0-%D0%B2-java-%D1%87%D1%82%D0%BE-%D1%8D%D1%82%D0%BE-%D1%82%D0%B0%D0%BA%D0%BE%D0%B5-%D0%B8-%D0%BA%D0%B0%D0%BA-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82-%D0%B2-jvm-25bb2570b44c}{Подробнее тут}
Сборка мусора в Java~---~это процесс, с помощью которого программы Java автоматически управляют памятью. Java-программы компилируются в байт-код, который запускается на виртуальной машине Java (JVM).

Когда Java-программы выполняются на JVM, объекты создаются в куче, которая представляет собой часть памяти, выделенную для них.

Пока Java-приложение работает, в нем создаются и запускаются новые объекты. В конце концов некоторые объекты перестают быть нужны. Можно сказать, что в любой момент времени память кучи состоит из двух типов объектов:
\begin{enumerate}
	\item Живые~---~эти объекты используются, на них ссылаются откуда-то еще.
	\item Мертвые~---~эти объекты больше нигде не используются, ссылок на них нет.
\end{enumerate}

Сборщик мусора находит эти неиспользуемые объекты и удаляет их, чтобы освободить память.

\textbf{Этапы сборки мусора в Java:}
\begin{enumerate}
	\item Пометка объектов как живых
	\item Зачистка мертвых объектов
	\item Компактное расположение оставшихся объектов в памяти
\end{enumerate} 

\textbf{Сбор мусора по поколениям.}

Сборщики мусора в Java реализуют стратегию сбора мусора поколений, которая классифицирует объекты по возрасту.

Область памяти кучи в JVM разделена на три секции:
\begin{enumerate}
	\item \emph{Молодое поколение.} 
	Вновь созданные объекты начинаются в молодом поколении. Молодое поколение далее подразделяется на две категории.
		\begin{enumerate}
			\item Пространство Эдема~---~все новые объекты начинают здесь, и им выделяется начальная память.
			\item Пространства выживших (FromSpace и ToSpace)~---~объекты перемещаются сюда из Эдема после того, как пережили один цикл сборки мусора.
		\end{enumerate}
	
	\item \emph{Старшее поколение.}
	Объекты-долгожители в конечном итоге переходят из молодого поколения в старшее. Оно также известно как штатное поколение и содержит объекты, которые долгое время оставались в пространствах выживших.
	\item \emph{Постоянное поколение} (\emph{Мета-пространство}, начиная с Java 8).
	Метаданные, такие как классы и методы, хранятся в постоянном поколении. JVM заполняет его во время выполнения на основе классов, используемых приложением. Классы, которые больше не используются, могут переходить из постоянного поколения в мусор.
	
	Начиная с Java 8, на смену пространству постоянного поколения (PermGen) приходит пространство памяти MetaSpace. Реализация отличается от PermGen — это пространство кучи теперь изменяется автоматически.
\end{enumerate}


\subsection{Java. Java Reflections. Работа с метаинформацией классов в процессе исполнения программ}

Java Reflection API~---~это программный интерфейс в языке Java, который позволяет приложениям анализировать свои компоненты и программное окружение, изменять собственное поведение и структуру. Позволяет исследовать информацию о полях, методах и конструкторах классов.

С помощью механизма рефлексии можно обрабатывать типы, которые отсутствовали при компиляции, но появились во время выполнения программы. Рефлексия и наличие логически целостной модели выдачи информации об ошибках позволяют создавать корректный динамический код. 

Помимо самомодификации, API способен проводить самопроверку и самоклонирование. Чаще всего рефлексию Java используют:
\begin{enumerate}
	\item для получения информации о классах, интерфейсах, функциях, конструкторах, методах и модулях;
	\item изменения имен функций и классов во время выполнения программы;
	\item создания новых экземпляров классов;
	\item анализа и исполнения кода, поступающего из программного окружения;
	\item преобразования классов из одного типа в другой;
	\item создания массивов данных и манипуляций с ними;
	\item установления значений полей объектов по именам;
	\item получения доступа к переменным и методам, включая приватные, и к внешним классам;
	\item вызова методов объектов по именам.
\end{enumerate}
\href{https://blog.skillfactory.ru/glossary/java-reflection-api/#:~:text=Java%20Reflection%20API%20%E2%80%94%20%D1%8D%D1%82%D0%BE%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D0%B9,%D0%BF%D0%BE%D0%BB%D1%8F%D1%85%2C%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B0%D1%85%20%D0%B8%20%D0%BA%D0%BE%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D0%BE%D1%80%D0%B0%D1%85%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%BE%D0%B2.}{Подробнее + пример}
\subsection{Метапрограммирование. Шаблоны и Generics. Частичная специализация шаблонов.}

\subsubsection{Метапрограммирование}

\textbf{Метапрограммирование}~---~создание программ, которые создают другие программы как результат своей работы (либо — частный случай — изменяющие или дополняющие себя во время выполнения).

Метапрограммирование можно разделить на 2 направления: на стадии компиляции (генерация кода) и на стадии выполнения (самомодифицирующийся код).

Первое направление позволяет получить программу при меньших затратах времени и усилий, чем если бы программист писал её вручную. Второе — расширяет возможности программиста.

\textbf{Генерация кода} (это не обязательно)

При этом подходе код программы не пишется вручную, а создается автоматически программой-генератором на основе другой, более простой программы.
Такой подход приобретает смысл, если при программировании вырабатываются различные дополнительные правила (более высокоуровневые парадигмы, выполнение требований внешних библиотек, стереотипные методы реализации определенных функций и пр.). При этом часть кода теряет содержательный смысл и становится лишь механическим выполнением правил. Когда эта часть становится значительной, возникает мысль задавать вручную лишь содержательную часть, а остальное добавлять автоматически. Это и проделывает генератор. Реализуется 2 основными методами:

\begin{enumerate}
	\item  \emph{Шаблоны} (наиболее известные случаи применения — препроцессор C и шаблоны в C++). Решают задачу, если соблюдение «правил» сводится к вставке в программу повторяющихся (или почти повторяющихся) кусков кода. Помимо этого, обладают еще рядом достоинств: например, помогают повторному использованию.
	\item \emph{Внешнеязыковые средства} (пример: генераторы синтаксических и лексических анализаторов lex, yacc, bison). Применяются в случаях, если простых средств вроде шаблонов недостаточно. Язык генератора составляется так, чтобы автоматически или с минимальными усилиями со стороны программиста реализовывать правила парадигмы или необходимые специальные функции. Фактически, это — более высокоуровневый язык программирования, а генератор — не что иное, как транслятор.
	Генераторы пишутся, как правило, для создания специализированных программ, в которых очень значительная часть стереотипна, либо для реализации сложных парадигм (таких, как паттерны проектирования).
\end{enumerate}

\textbf{Самомодифицирующийся код} (это не обязательно)

Возможность изменять или дополнять себя во время выполнения превращает программу в виртуальную машину. Хотя такая возможность существовала уже давно на уровне машинных кодов (и активно использовалась, например, при создании полиморфных вирусов), с метапрограммированием обычно связывают перенос подобных технологий в высокоуровневые языки. Основные методы реализации:
\begin{enumerate}
	\item Интроспекция — представление внутренних структур языка в виде переменных встроенных типов с возможностью доступа к ним из программы. Позволяет во время выполнения смотреть, создавать и изменять определения типов, стек вызовов, обращаться к переменной по имени, получаемому динамически и пр.
	Например, Пространство имён System.Reflection и тип System.Type в .NET; классы Class, Method, Field в Java; представление пространств имен и определений типов через встроенные типы данных в Python
	\item  Интерпретация произвольного кода, представленного в виде строки.
	Существует естественным образом во множестве интерпретируемых языков, например eval() в PHP.
	Для C++ есть библиотека, позволяющая «на лету» компилировать и генерировать исполняемый код (используется урезанный компилятор gcc).
	Принципиальный недостаток технологий этого направления — неприменимость к компилируемым языкам. Можно ввести в такой язык интерпретатор, как в вышеуказанной библиотеке для С++, но это
	практически сведет на нет главное преимущество данных языков — производительность.
\end{enumerate}

\subsubsection{Шаблоны и Generics. Частичная специализация шаблонов.}

\textbf{Шаблоны:}

В языке C++ обобщённое программирование основывается на понятии «шаблон», обозначаемом ключевым словом template.

\begin{lstlisting}[language=C++]
template < typename T> T max (T x , T y ) {
	if (x < y)
		return y;
	else
		return x;
}
\end{lstlisting}

Интересное применение нашли шаблоны в языке C++. Оказалось, что шаблоны в этом языке являются тьюринг-полным функциональном языком. Другими словами на шаблонах С++ можно написать программу, реализующую произвольный алгоритм, и эта программа выполнится в момент компиляции.

К примеру, можно предпосчитать $50$-е число Фибоначчи. Тогда во время выполнения программы не придется тратить время на его вычисления. Одной интересной особенностью такого программирования на шаблонах, является встроенный механизм мемоизации (сохранения результата вычисления функции). Это значит, что рекурсивный алгоритм для вычисления k -го числа Фибоначчи работающий «в лоб» сделает порядка k операций (вместо ожидаемых 2k).

\begin{lstlisting}[language=C++]
template <int i> struct fib { 
	static const int val = fib<i - 1>::val + fib<i - 2>::val;
};
template <> struct fib <1> { static const int val = 1; };
template <> struct fib <2> { static const int val = 1; };
\end{lstlisting}

Но это не самое интересное: программирование на шаблонах С++ позволяет общаться с типом как с обычным объектом. К примеру, можно составить список типов, удалить из него все встроенные типы, а
из оставшегося списка создать объект, который будет унаследован от всех типов из данного списка. Для такого метапрограммирования была написана специальная библиотека MPL (MetaProgramming Library).

\textbf{Generics}

Язык Java предоставляет средства обобщённого программирования, синтаксически основанные на C++. В Java generics (параметризованные типы или родовые типы) имеют мнимое сходство с шаблонами C++ как
по синтаксису, так и по ожидаемому месту их применения (например, в качестве контейнерных классов).

Но это сходство только поверхностное — родовые типы в языке программирования Java почти полностью реализуются в компиляторе, который выполняет проверку типов и выявление типа (type inference)
и, затем, генерирует обычные не параметризованные байткоды. Такая техника реализации, называемая стиранием (когда компилятор использует информацию о родовом типе для контроля типов и удаляет ее перед генерированием байткода), имеет неожиданные, а иногда и непонятные последствия. В то время как родовые типы являются большим шагом на пути к безопасности Java-классов, изучение их использования почти наверняка будет вызывать некоторую озадаченность (а иногда и мучения).

\textbf{Частичная специализация шаблонов}

Если у шаблона класса есть несколько параметров, то можно специализировать его только для одного или нескольких аргументов, оставляя другие неспециализированными. Иными словами, допустимо написать шаблон, соответствующий общему во всем, кроме тех параметров, вместо которых подставлены фактические типы или значения. Такой механизм носит название частичной специализации шаблона класса. Она может понадобиться при определении реализации, более подходящей для конкретного набора аргументов. Например unique\_ptr имеет частичную специализацию для массивов (T[]).

\begin{lstlisting}[language=C++]
	template <typename T>
	class unique_ptr;
	
	template <typename T>
	class unique_ptr<T[]>;
\end{lstlisting}
Частичная специализация шаблона класса — это тоже шаблон, но список параметров здесь отличается от соответствующего списка параметров общего шаблона.

\subsection{Функциональное программирование. Чистые объекты. Функторы. Аппликативы. Монада. Взаимодействие с внешним миром.}

\subsubsection{Функциональное программирование. Чистые объекты.}
\textbf{Функциональное программирование}~---~парадигма программирования, в которой процесс вычисления трактуется как вычисление значений функций в математическом понимании последних (в отличие от функций как подпрограмм в процедурном программировании).

\textbf{Чистыми} называют функции, которые не имеют побочных эффектов ввода-вывода и памяти (они зависят только от своих параметров и возвращают только свой результат). Чистые функции обладают несколькими полезными свойствами, многие из которых можно использовать для оптимизации кода:
\begin{enumerate}
	\item если результат чистой функции не используется, её вызов может быть удалён без вреда для других выражений;
	\item результат вызова чистой функции может быть мемоизирован, то есть сохранён в таблице значений вместе с аргументами вызова;
	\item если нет никакой зависимости по данным между двумя чистыми функциями, то порядок их вычисления можно поменять или распараллелить (говоря иначе, вычисление чистых функций удовлетворяет принципам потокобезопасности);
	\item если весь язык не допускает побочных эффектов, то можно использовать любую политику вычисления. Это предоставляет свободу компилятору комбинировать и реорганизовывать вычисление выражений в программе (например, исключить древовидные структуры).
\end{enumerate}

\subsubsection{Функторы}

\href{https://habr.com/ru/post/183150/}{Объяснение на пальцах}
\href{http://cmc-msu-ai.github.io/haskell-course/lecture/2013/09/07/functors.html}{Подробнее про функторы}
\textbf{Функтором} называется класс типов, который декларирует единственный метод «fmap». Интуитивно, «fmap» применяет функцию a -> b к значению типа f a, чтобы получить значение типа f b. С другой стороны, можно рассматривать «fmap» как функцию высшего порядка, преобразующую «простую» функцию a -> b в «составную» функцию f a -> f b. Важно отметить, что структура значения типа f после применения «fmap» должна оставаться неизменной.

\begin{lstlisting}[language=Haskell]
class Functor f where
	fmap :: (a -> b) -> f a -> f b
\end{lstlisting}

\subsubsection{Аппликативы.}
\href{http://cmc-msu-ai.github.io/haskell-course/lecture/2013/09/08/applicative-and-monad.html}{Подробнее тут}

Естественным продолжением класса Functor является класс \textbf{Applicative} (аппликативный функтор), определенный в модуле Control.Applicative:

\begin{lstlisting}[language=Haskell]
class Functor f => Applicative f where
	pure  :: a -> f a
	(<*>) :: f (a -> b) -> f a -> f b
\end{lstlisting}

\textbf{Законы:}
\begin{enumerate}
	\item Помещение тождественной функции в «чистый» контекст и применение к аргументу в контексте не меняет ни значение, ни контекст.
	\begin{lstlisting}[language=Haskell]
pure id <*> x == x
	\end{lstlisting}
	\item Применение чистой функции к чистому аргументу в контексте «по умолчанию» должно быть эквивалентно применению функции, а затем помещению результата в контекст.
	\begin{lstlisting}[language=Haskell]
pure f <*> pure x == pure (f x)
	\end{lstlisting}
	\item При применении функции u с побочными эффектами к чистому аргументу y порядок вычисления функции и аргумента неважен.
	\begin{lstlisting}[language=Haskell]
u <*> pure y == pure ($ y) <*> u
	\end{lstlisting}
	\item Некоторый аналог композиции для аппликативных функторов.
	\begin{lstlisting}[language=Haskell]
u <*> (v <*> w) == pure (.) <*> u <*> v <*> w
	\end{lstlisting}
\end{enumerate} 

\subsubsection{Монады}
\href{http://cmc-msu-ai.github.io/haskell-course/lecture/2013/09/08/applicative-and-monad.html}{Подробнее тут}
\begin{lstlisting}[language=Haskell]
class Monad m where
	return :: a -> m a
	(>>=) :: m a -> (a -> m b) -> mb
	(>>) :: m a -> m b -> m b
	m >> n = m >>= \_ -> n
	
	fail :: String -> m a
\end{lstlisting}

Функция return по типу очень напоминает функцию pure из класса Applicative. И, в действительности, return и есть pure, хоть и с не самым удачным названием (return в Haskell совсем не то же, что return в обычных императивных языках вроде C или Java). С математической точки зрения, любая монада является аппликативным функтором (но не наоборот). Но по историческим причинам,в описании класса это не указано.

Как следует из определения, операция ($>>$) является частным случаем операции ($>>=$)

Функция fail осталась в классе по историческим причинам, хотя никакого отношения к монадам реально не имеет.

\textbf{Законы:}
\begin{lstlisting}[language=Haskell]
return a >>= k = k a
m >>= return = m
m >>= (\x -> k x >>= h) = (m >>= k) >>= h
fmap f xs = xs >>= return . f = liftM f xs
\end{lstlisting}

\subsubsection{Взаимодействие с внешним миром.}
Взаимодействие с "внешним миром" (побочные эффекты вычислений) можно реализовать с помощью специальной монады IO.

Грубое приближение монады IO — это взятие пары с контекстом: "(a, RealWorld)". При операциях с монадой состояние RealWorld может меняться.

Есть операция return, погружающая объект в окружение IO («выпускающая во внешний мир»), а вот обратного преобразования нет.

\begin{lstlisting}[language=Haskell]
putChar :: Char -> IO () 
\end{lstlisting}~---~берёт символ и возвращает новый «мир», в котором растворился (был напечатан в консоли) этот символ.

\begin{lstlisting}[language=Haskell]
getChar :: IO Char
\end{lstlisting} --- мы можем получить символ из внешнего мира, но только внутри монады.

Достать его из монады мы не можем, но можем работать сним внутри IO с помощью $>>=$.

В процессе выполнения программы, содержащей IO, объекты типов IO a остаются временно невычисленными, как задумки.

Например, если мы где-то напишем putChar ' a' , то символ не будет тут же напечатан.

Вместо этого нужно дождаться, пока соберётся «главный» объект типа IO (), и уже при его вычислении все операции с внешним миром будут выполнены, причём в правильном порядке.

\subsection{Операционные системы. Процессы: вытесняющая и кооперативная многозадачность, планировщики, многопроцессорные машины.}

\textbf{Кооперативная многозадачность}.

Тип многозадачности, при котором фоновые задачи выполняются только во время простоя основного процесса и только в том случае, если на это получено разрешение основного процесса.

Кооперативную многозадачность можно назвать многозадачностью “второй ступени” поскольку она использует более передовые методы, чем простое переключение задач, реализованное многими известными программами (например, МS-DOS shell из МS-DOS 5.0 при простом переключении активная программа получает все процессорное время, а фоновые приложения полностью замораживаются). При кооперативной многозадачности приложение может захватить фактически столько процессорного времени, сколько оно считает нужным. Все приложения делят процессорное время, периодически передавая управление следующей задаче.

\textbf{ Вытесняющая многозадачность}.

Вид многозадачности, в котором операционная система сама передает управление от одной выполняемой программы другой. Распределение процессорного времени осуществляется планировщиком процессов. Этот вид многозадачности обеспечивает более быстрый отклик на действия пользователя.

Вытесняющая многозадачность~---~это вид многозадачности при котором планирование процессов основывается на абсолютных приоритетах. Процесс с меньшим приоритетом (например пользовательская программа) может быть вытеснен при его выполнении более приоритетным процессом (например системной или диагностической программой). Иногда этот вид многозадачности называют приоритетным.

Каждая работающая программа имеет свое защищенное адресное пространство. Многопоточное выполнение отдельных задач позволяет при задержке в выполнении одного потока не останавливать задачу полностью, а работать со следующим потоком.

\textbf{Планировщик.}

\href{https://habr.com/ru/post/154609/}{Подробнее тут}

Планировщик~---~часть операционной системы, которая отвечает за (псевдо)параллельное выполнения задач, потоков, процессов. Планировщик выделяет потокам процессорное время, память, стек и прочие ресурсы. Планировщик может принудительно забирать управление у потока (например по таймеру или при появлении потока с большим приоритетом), либо просто ожидать пока поток сам явно(вызовом некой системной процедуры) или неявно(по завершении) отдаст управление планировщику.
Первый вариант работы планировщика называется реальным или вытесняющим(preemptive), второй, соответственно, не вытесняющим (non-preemptive).

\textbf{Многопроцессорные машины.}

\href{https://docstore.mik.ua/skbd/glava_10.htm}{Подробнее тут}

\href{https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81%D0%BE%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C}{В Википедии тоже хорошо написано}

Любая вычислительная система (будь то супер-ЭВМ или персональный компьютер) достигает своей наивысшей производительности благодаря использованию высокоскоростных элементов и параллельному выполнению большого числа операций. Именно возможность параллельной работы различных устройств системы (работы с перекрытием) является основой ускорения основных операций. 

Параллельные ЭВМ часто подразделяются по классификации Флинна на машины типа SIMD (Single Instruction Multiple Data - с одним потоком команд при множественном потоке данных) и MIMD (Multiple Instruction Multiple Data - с множественным потоком команд при множественном потоке данных). Как и любая другая, приведенная выше классификация несовершенна: существуют машины прямо в нее не попадающие, имеются также важные признаки, которые в этой классификации не учтены. В частности, к машинам типа SIMD часто относят векторные процессоры, хотя их высокая производительность зависит от другой формы параллелизма - конвейерной организации машины. Многопроцессорные векторные системы, типа Cray Y-MP, состоят из нескольких векторных процессоров и поэтому могут быть названы MSIMD (Multiple SIMD).

\subsection{Операционные системы. Виртуальная память: MMU, TLB, таблицы страниц, аллокаторы и менеджеры виртуальной памяти.}

\subsubsection{Виртуальная память: MMU, TLB.}

\href{https://habr.com/ru/post/211150/}{Подробнее тут}

Блок управления памятью или устройство управления памятью memory management unit, MMU)~---~компонент аппаратного обеспечения компьютера, отвечающий за управление доступом к памяти, запрашиваемым центральным процессором.

Его функции заключаются в трансляции адресов виртуальной памяти в адреса физической памяти (то есть управление виртуальной памятью), защите памяти, управлении кэш-памятью, арбитражем шины и, в более простых компьютерных архитектурах (особенно 8-битных), переключением блоков памяти. 

Принцип работы современных MMU основан на разделении виртуального адресного пространства (одномерного массива адресов, используемых центральным процессором) на участки одинакового, как правило, несколько килобайт, хотя, возможно, и существенно большего, размера, равного степени 2, называемые страницами. Младшие n бит адреса (смещение внутри страницы) остаются неизменными. Старшие биты адреса представляют собой номер (виртуальной) страницы. MMU обычно преобразует номера виртуальных страниц в номера физических страниц, используя буфер ассоциативной трансляции (Translation Lookaside Buffer, TLB).

Если преобразование при помощи TLB невозможно, включается более медленный механизм преобразования, основанный на специфическом аппаратном обеспечении или на программных системных структурах. Данные в этих структурах, как правило, называются элементами таблицы страниц (page table entries (PTE)), а сами структуры — таблицами страниц (англ. page table (PT)). Конкатенация номера физической страницы со смещением внутри страницы даёт физический адрес.

Элементы PTE или TLB могут также содержать дополнительную информацию: бит признака записи в страницу ( dirty bit), время последнего доступа к странице (accessed bit), какие процессы (пользовательские (user mode) или системные (supervisor mode)) могут читать или записывать данные в страницу, необходимо ли кэшировать страницу.

\subsubsection{Таблицы страниц.}

\href{https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B8%D1%86}{Википедия}

Таблица страниц~---~это структура данных, используемая системой виртуальной памяти в операционной системе компьютера для хранения сопоставления между виртуальным адресом и физическим адресом. Виртуальные адреса используются выполняющимся процессом, в то время как физические адреса используются аппаратным обеспечением, или, более конкретно, подсистемой ОЗУ. Таблица страниц является ключевым компонентом преобразования виртуальных адресов, который необходим для доступа к данным в памяти.

\subsubsection{Аллокаторы.}

\href{https://habr.com/ru/post/505632/}{Подробнее тут}

Аллокатор или распределитель памяти в языке программирования C++ ~---~ специализированный класс, реализующий и инкапсулирующий малозначимые (с прикладной точки зрения) детали распределения и освобождения ресурсов компьютерной памяти.

Концептуально выделяется пять основных операции, которые можно осуществить над аллокатором:
\begin{enumerate}
	\item \emph{create}~---~создает аллокатор и отдает ему в распоряжение некоторый объем памяти;
	\item \emph{allocate}~---~выделяет блок определенного размера из области памяти, которым распоряжается аллокатор;
	\item \emph{deallocate}~---~освобождает определенный блок;
	\item \emph{free}~---~освобождает все выделенные блоки из памяти аллокатора (память, выделенная аллокатору, не освобождается);
	\item \emph{destroy}~---~уничтожает аллокатор с последующим освобождением памяти, выделенной аллокатору.
\end{enumerate}

\subsubsection{Менеджеры виртуальной памяти.}

Менеджер виртуальной памяти (далее просто «менеджер памяти») ~---~ часть операционной системы, благодаря которой можно адресовать память большую, чем объем физической памяти (ОЗУ).

Благодаря виртуальной памяти можно запускать множество ресурсоёмких приложений, требующих большого объёма ОЗУ. Максимальный объём виртуальной памяти, который можно получить, используя 24-битную адресацию, — 16 мегабайт. С помощью 32-битной адресации можно адресовать до 4 ГБ виртуальной памяти. А 64-битная адресация позволяет работать уже с 16 эксабайтами памяти.

Применение механизма виртуальной памяти позволяет:

\begin{enumerate}
	\item упростить адресацию памяти клиентским программным обеспечением;
	\item рационально управлять оперативной памятью компьютера (хранить в ней только активно используемые области памяти);
	\item изолировать процессы друг от друга (процесс полагает, что монопольно владеет всей памятью).
\end{enumerate}

\subsection{Операционные системы. Файловые системы (UNIX): файлы и директории, inode, контроль доступа. Файловые системы (реализация в ядре): VFS, блочные устройства, планировщик IO}

\subsection{Параллельное программирование. Свойства прогресса (lock-freedom, waitfreedom). Свойства корректности (linearizability, sequential consistency). Универсальная конструкция.}

\subsection{Параллельное программирование. Консенсусное число. Стек Трайбера. Очередь Майкла-Скотта.}

\subsection{Компьютерные сети. Модель OSI. Устройства коммутации и маршрутизации. Протоколы Ethernet, IP, TCP, UDP.}

\href{https://www.dropbox.com/sh/4st5b16mvdf8gkj/AAAI9sbKs_C3TFgRbqWGrAeca/Programming/16%20%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8.pdf?dl=0}{dropbox}

\subsection{Компьютерные сети. DHCP, DNS, NAT. Интернет-протоколы.}

\subsection{Базы данных. Типы баз данных. Реляционные БД. Нормальные формы РБД. Язык SQL}

\href{https://www.dropbox.com/sh/4st5b16mvdf8gkj/AACJWcRNv0lFmPzhlkMSxN7Va/Programming/12%20%D0%91%D0%B0%D0%B7%D1%8B%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.pdf?dl=0}{dropbox}

\subsection{Базы данных. Реализация реляционных СУБД. Индексы, применение индексов. Транзакции, свойства транзакций. Блокировки, типы блокировок.}

\href{https://www.dropbox.com/sh/4st5b16mvdf8gkj/AADo-NrUSL5vJLWDfeAFWwy1a/Programming/13%20%D0%A0%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D0%A1%D0%A3%D0%91%D0%94.pdf?dl=0}{dropbox}

\subsection{Шаблоны проектирования, их применение. Классификация шаблонов проектирования. Примеры шаблонов проектирования.}

\href{https://www.dropbox.com/sh/4st5b16mvdf8gkj/AACWB4gDVx73cjp5xudo1nGma/Programming/18%20%D0%A8%D0%B0%D0%B1%D0%BB%D0%BE%D0%BD%D1%8B%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F.pdf?dl=0}{dropbox}

\subsection{Теория кодирования. Блоковые коды и их параметры. Критерии декодирования и метрики. Границы Хемминга и Варшамова-Гилберта.}

\subsection{Теория кодирования. Линейные коды. Границы Синглтона, ВаршамоваГилберта и Грайсмера. Вероятность ошибки декодирования и необнаружения ошибки.}

\subsection{Машинное обучение. Понятие машинного обучения в искусственном интеллекте. Классификация задач машинного обучения, их примеры и особенности.}

\subsection{Машинное обучение. Многослойная нейронная сеть и алгоритм обратного распространения ошибок. Методы инициализации и методы оптимизации в нейронных сетях. Архитектуры сетей, их применение}

\end{document}
